* 
* ==> Audit <==
* |------------|-------------------------------|----------|--------|---------|---------------------|---------------------|
|  Command   |             Args              | Profile  |  User  | Version |     Start Time      |      End Time       |
|------------|-------------------------------|----------|--------|---------|---------------------|---------------------|
| start      |                               | minikube | truong | v1.32.0 | 19 Dec 23 10:16 +07 | 19 Dec 23 10:19 +07 |
| start      |                               | minikube | truong | v1.32.0 | 19 Dec 23 16:04 +07 | 19 Dec 23 16:05 +07 |
| start      |                               | minikube | truong | v1.32.0 | 20 Dec 23 08:15 +07 | 20 Dec 23 08:20 +07 |
| addons     | enable registry               | minikube | truong | v1.32.0 | 20 Dec 23 08:48 +07 | 20 Dec 23 08:48 +07 |
| ip         |                               | minikube | truong | v1.32.0 | 20 Dec 23 08:49 +07 | 20 Dec 23 08:49 +07 |
| ip         |                               | minikube | truong | v1.32.0 | 20 Dec 23 09:00 +07 | 20 Dec 23 09:00 +07 |
| dashboard  |                               | minikube | truong | v1.32.0 | 20 Dec 23 09:08 +07 |                     |
| addons     | list                          | minikube | truong | v1.32.0 | 20 Dec 23 09:20 +07 | 20 Dec 23 09:20 +07 |
| docker-env |                               | minikube | truong | v1.32.0 | 20 Dec 23 09:23 +07 | 20 Dec 23 09:23 +07 |
| docker-env |                               | minikube | truong | v1.32.0 | 20 Dec 23 09:52 +07 | 20 Dec 23 09:52 +07 |
| docker-env |                               | minikube | truong | v1.32.0 | 20 Dec 23 09:54 +07 | 20 Dec 23 09:54 +07 |
| image      | ls                            | minikube | truong | v1.32.0 | 20 Dec 23 09:56 +07 | 20 Dec 23 09:56 +07 |
| image      | load localhost:5000/go-base:2 | minikube | truong | v1.32.0 | 20 Dec 23 10:02 +07 | 20 Dec 23 10:03 +07 |
| image      | ls                            | minikube | truong | v1.32.0 | 20 Dec 23 10:03 +07 | 20 Dec 23 10:03 +07 |
| ssh        | -- docker ps                  | minikube | truong | v1.32.0 | 20 Dec 23 10:06 +07 | 20 Dec 23 10:06 +07 |
| image      | ls                            | minikube | truong | v1.32.0 | 20 Dec 23 10:20 +07 | 20 Dec 23 10:20 +07 |
|------------|-------------------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/20 08:15:40
Running on machine: truong-System-Product-Name
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1220 08:15:40.642169    3066 out.go:296] Setting OutFile to fd 1 ...
I1220 08:15:40.642241    3066 out.go:348] isatty.IsTerminal(1) = true
I1220 08:15:40.642244    3066 out.go:309] Setting ErrFile to fd 2...
I1220 08:15:40.642247    3066 out.go:348] isatty.IsTerminal(2) = true
I1220 08:15:40.642364    3066 root.go:338] Updating PATH: /home/truong/.minikube/bin
W1220 08:15:40.642545    3066 root.go:314] Error reading config file at /home/truong/.minikube/config/config.json: open /home/truong/.minikube/config/config.json: no such file or directory
I1220 08:15:40.643102    3066 out.go:303] Setting JSON to false
I1220 08:15:40.643865    3066 start.go:128] hostinfo: {"hostname":"truong-System-Product-Name","uptime":208,"bootTime":1703034733,"procs":299,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.2.0-39-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"383fe771-5de0-47a1-bbbe-3ac4797a3bf7"}
I1220 08:15:40.643901    3066 start.go:138] virtualization: kvm host
I1220 08:15:40.645321    3066 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I1220 08:15:40.647664    3066 notify.go:220] Checking for updates...
I1220 08:15:40.648125    3066 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1220 08:15:40.648176    3066 driver.go:378] Setting default libvirt URI to qemu:///system
I1220 08:15:40.666085    3066 virtualbox.go:136] virtual box version: 6.1.48r159471
I1220 08:15:40.667331    3066 out.go:177] ‚ú®  Using the virtualbox driver based on existing profile
I1220 08:15:40.669319    3066 start.go:298] selected driver: virtualbox
I1220 08:15:40.669323    3066 start.go:902] validating driver "virtualbox" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/truong:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1220 08:15:40.669376    3066 start.go:913] status for virtualbox: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.48r159471
}
I1220 08:15:40.670686    3066 cni.go:84] Creating CNI manager for ""
I1220 08:15:40.670696    3066 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1220 08:15:40.670707    3066 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/truong:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1220 08:15:40.670889    3066 iso.go:125] acquiring lock: {Name:mkd28c45b7dcf54993cc4a08cc84b844f4c5cd22 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1220 08:15:40.672304    3066 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1220 08:15:40.673378    3066 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1220 08:15:40.673403    3066 preload.go:148] Found local preload: /home/truong/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1220 08:15:40.673407    3066 cache.go:56] Caching tarball of preloaded images
I1220 08:15:40.673478    3066 preload.go:174] Found /home/truong/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1220 08:15:40.673498    3066 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1220 08:15:40.673560    3066 profile.go:148] Saving config to /home/truong/.minikube/profiles/minikube/config.json ...
I1220 08:15:40.673699    3066 start.go:365] acquiring machines lock for minikube: {Name:mk7b4ec7b183afdfd3197829f1003b959c8b3459 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1220 08:15:40.673731    3066 start.go:369] acquired machines lock for "minikube" in 24.331¬µs
I1220 08:15:40.673742    3066 start.go:96] Skipping create...Using existing machine configuration
I1220 08:15:40.673745    3066 fix.go:54] fixHost starting: 
I1220 08:15:40.673881    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:15:40.906186    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2023-12-19T03:18:56.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I1220 08:15:40.906198    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:40.906232    3066 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1220 08:15:40.906242    3066 fix.go:128] unexpected machine state, will restart: <nil>
I1220 08:15:40.907420    3066 out.go:177] üîÑ  Restarting existing virtualbox VM for "minikube" ...
I1220 08:15:40.909409    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:15:40.955444    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2023-12-19T03:18:56.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I1220 08:15:40.955455    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:40.955492    3066 main.go:141] libmachine: Check network to re-create if needed...
I1220 08:15:40.955501    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I1220 08:15:40.991206    3066 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     
IPV6NetworkMaskPrefixLength: 0
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Down
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I1220 08:15:40.991216    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:40.991428    3066 main.go:141] libmachine: Searching for hostonly interface for IPv4: 192.168.59.1 and Mask: ffffff00
I1220 08:15:40.991436    3066 main.go:141] libmachine: Found: vboxnet0
I1220 08:15:40.991443    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list dhcpservers
I1220 08:15:41.022409    3066 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.19
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I1220 08:15:41.022418    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.022450    3066 main.go:141] libmachine: Removing orphan DHCP servers...
I1220 08:15:41.022456    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I1220 08:15:41.059194    3066 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     
IPV6NetworkMaskPrefixLength: 0
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Down
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I1220 08:15:41.059200    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.059237    3066 main.go:141] libmachine: Adding/Modifying DHCP server "192.168.59.21" with address range "192.168.59.100" - "192.168.59.254"...
I1220 08:15:41.059244    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list dhcpservers
I1220 08:15:41.090387    3066 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.19
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I1220 08:15:41.090397    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.090446    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage dhcpserver modify --netname HostInterfaceNetworking-vboxnet0 --ip 192.168.59.21 --netmask 255.255.255.0 --lowerip 192.168.59.100 --upperip 192.168.59.254 --enable
I1220 08:15:41.135302    3066 main.go:141] libmachine: STDOUT:
{
}
I1220 08:15:41.135310    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.135329    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --nic2 hostonly --nictype2 virtio --nicpromisc2 deny --hostonlyadapter2 vboxnet0 --cableconnected2 on
I1220 08:15:41.172897    3066 main.go:141] libmachine: STDOUT:
{
}
I1220 08:15:41.172904    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.172993    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --natpf1 delete ssh
I1220 08:15:41.207454    3066 main.go:141] libmachine: STDOUT:
{
}
I1220 08:15:41.207461    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.207474    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage modifyvm minikube --natpf1 ssh,tcp,127.0.0.1,43605,,22
I1220 08:15:41.241635    3066 main.go:141] libmachine: STDOUT:
{
}
I1220 08:15:41.241645    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.241658    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage startvm minikube --type headless
I1220 08:15:41.523099    3066 main.go:141] libmachine: STDOUT:
{
Waiting for VM "minikube" to power on...
VM "minikube" has been successfully started.
}
I1220 08:15:41.523107    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:15:41.523115    3066 main.go:141] libmachine: Checking vm logs: /home/truong/.minikube/machines/minikube/minikube/Logs/VBox.log
I1220 08:15:41.523352    3066 main.go:141] libmachine: Waiting for an IP...
I1220 08:15:41.523357    3066 main.go:141] libmachine: Getting to WaitForSSH function...
I1220 08:15:41.523398    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:15:41.523765    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:15:41.523769    3066 main.go:141] libmachine: About to run SSH command:
exit 0
I1220 08:16:11.922895    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1220 08:16:11.922922    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:11.974654    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:11.974663    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:11.974706    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.022598    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.022609    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.022717    3066 main.go:141] libmachine: Host-only MAC: 080027cee20a

I1220 08:16:12.022736    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.022984    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.022989    3066 main.go:141] libmachine: About to run SSH command:
ip addr show
I1220 08:16:12.099436    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1220 08:16:12.099461    3066 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1220 08:16:12.099473    3066 main.go:141] libmachine: IP is 192.168.59.100
I1220 08:16:12.099481    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.148399    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.148415    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.148455    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.197606    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.197616    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.197729    3066 main.go:141] libmachine: Host-only MAC: 080027cee20a

I1220 08:16:12.197749    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.197996    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.198002    3066 main.go:141] libmachine: About to run SSH command:
ip addr show
I1220 08:16:12.272852    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1220 08:16:12.272879    3066 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1220 08:16:12.272893    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage list hostonlyifs
I1220 08:16:12.304044    3066 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::800:27ff:fe00:0
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I1220 08:16:12.304059    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.304280    3066 main.go:141] libmachine: Found: vboxnet0
I1220 08:16:12.304472    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.353901    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.353913    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.353955    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.402625    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.402638    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.402744    3066 main.go:141] libmachine: Host-only MAC: 080027cee20a

I1220 08:16:12.402768    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.403010    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.403015    3066 main.go:141] libmachine: About to run SSH command:
ip addr show
I1220 08:16:12.477709    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1220 08:16:12.477749    3066 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1220 08:16:12.477833    3066 profile.go:148] Saving config to /home/truong/.minikube/profiles/minikube/config.json ...
I1220 08:16:12.477985    3066 machine.go:88] provisioning docker machine ...
I1220 08:16:12.477995    3066 buildroot.go:166] provisioning hostname "minikube"
I1220 08:16:12.478013    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.478261    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.478267    3066 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1220 08:16:12.562758    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1220 08:16:12.562797    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.563086    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.563094    3066 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1220 08:16:12.639868    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1220 08:16:12.639879    3066 buildroot.go:172] set auth options {CertDir:/home/truong/.minikube CaCertPath:/home/truong/.minikube/certs/ca.pem CaPrivateKeyPath:/home/truong/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/truong/.minikube/machines/server.pem ServerKeyPath:/home/truong/.minikube/machines/server-key.pem ClientKeyPath:/home/truong/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/truong/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/truong/.minikube}
I1220 08:16:12.639895    3066 buildroot.go:174] setting up certificates
I1220 08:16:12.639902    3066 provision.go:83] configureAuth start
I1220 08:16:12.639925    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.689490    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.689503    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.689541    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:12.737594    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:12.737606    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:12.737747    3066 main.go:141] libmachine: Host-only MAC: 080027cee20a

I1220 08:16:12.737768    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.738015    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.738020    3066 main.go:141] libmachine: About to run SSH command:
ip addr show
I1220 08:16:12.813147    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86388sec preferred_lft 86388sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 588sec preferred_lft 588sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1220 08:16:12.813174    3066 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86388sec preferred_lft 86388sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 588sec preferred_lft 588sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1220 08:16:12.813184    3066 provision.go:138] copyHostCerts
I1220 08:16:12.820926    3066 exec_runner.go:144] found /home/truong/.minikube/ca.pem, removing ...
I1220 08:16:12.820934    3066 exec_runner.go:203] rm: /home/truong/.minikube/ca.pem
I1220 08:16:12.820965    3066 exec_runner.go:151] cp: /home/truong/.minikube/certs/ca.pem --> /home/truong/.minikube/ca.pem (1078 bytes)
I1220 08:16:12.821114    3066 exec_runner.go:144] found /home/truong/.minikube/cert.pem, removing ...
I1220 08:16:12.821117    3066 exec_runner.go:203] rm: /home/truong/.minikube/cert.pem
I1220 08:16:12.821131    3066 exec_runner.go:151] cp: /home/truong/.minikube/certs/cert.pem --> /home/truong/.minikube/cert.pem (1119 bytes)
I1220 08:16:12.821264    3066 exec_runner.go:144] found /home/truong/.minikube/key.pem, removing ...
I1220 08:16:12.821267    3066 exec_runner.go:203] rm: /home/truong/.minikube/key.pem
I1220 08:16:12.821280    3066 exec_runner.go:151] cp: /home/truong/.minikube/certs/key.pem --> /home/truong/.minikube/key.pem (1675 bytes)
I1220 08:16:12.821429    3066 provision.go:112] generating server cert: /home/truong/.minikube/machines/server.pem ca-key=/home/truong/.minikube/certs/ca.pem private-key=/home/truong/.minikube/certs/ca-key.pem org=truong.minikube san=[192.168.59.100 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1220 08:16:12.902808    3066 provision.go:172] copyRemoteCerts
I1220 08:16:12.902856    3066 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1220 08:16:12.902864    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:16:12.945554    3066 ssh_runner.go:362] scp /home/truong/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1220 08:16:12.959124    3066 ssh_runner.go:362] scp /home/truong/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1220 08:16:12.972710    3066 ssh_runner.go:362] scp /home/truong/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1220 08:16:12.986278    3066 provision.go:86] duration metric: configureAuth took 346.370257ms
I1220 08:16:12.986289    3066 buildroot.go:189] setting minikube options for container-runtime
I1220 08:16:12.986423    3066 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1220 08:16:12.986447    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:12.986709    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:12.986714    3066 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1220 08:16:13.063486    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1220 08:16:13.063496    3066 buildroot.go:70] root file system type: tmpfs
I1220 08:16:13.063606    3066 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1220 08:16:13.063647    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:13.063968    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:13.064071    3066 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1220 08:16:13.149117    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1220 08:16:13.149166    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:13.149408    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:13.149418    3066 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1220 08:16:13.782174    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I1220 08:16:13.782184    3066 machine.go:91] provisioned docker machine in 1.304194749s
I1220 08:16:13.782190    3066 start.go:300] post-start starting for "minikube" (driver="virtualbox")
I1220 08:16:13.782199    3066 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1220 08:16:13.782243    3066 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1220 08:16:13.782253    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:16:13.824987    3066 ssh_runner.go:195] Run: cat /etc/os-release
I1220 08:16:13.827230    3066 info.go:137] Remote host: Buildroot 2021.02.12
I1220 08:16:13.827240    3066 filesync.go:126] Scanning /home/truong/.minikube/addons for local assets ...
I1220 08:16:13.827395    3066 filesync.go:126] Scanning /home/truong/.minikube/files for local assets ...
I1220 08:16:13.827518    3066 start.go:303] post-start completed in 45.324335ms
I1220 08:16:13.827523    3066 fix.go:56] fixHost completed within 33.153778458s
I1220 08:16:13.827542    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:13.827793    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:13.827798    3066 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I1220 08:16:13.904264    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1703034973.902497765

I1220 08:16:13.904272    3066 fix.go:206] guest clock: 1703034973.902497765
I1220 08:16:13.904278    3066 fix.go:219] Guest: 2023-12-20 08:16:13.902497765 +0700 +07 Remote: 2023-12-20 08:16:13.827524529 +0700 +07 m=+33.260937394 (delta=74.973236ms)
I1220 08:16:13.904306    3066 fix.go:190] guest clock delta is within tolerance: 74.973236ms
I1220 08:16:13.904311    3066 start.go:83] releasing machines lock for "minikube", held for 33.230573837s
I1220 08:16:13.904332    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:13.955716    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:13.955728    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:13.955768    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:16:14.003765    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:16:14.003779    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:16:14.003892    3066 main.go:141] libmachine: Host-only MAC: 080027cee20a

I1220 08:16:14.003911    3066 main.go:141] libmachine: Using SSH client type: native
I1220 08:16:14.004154    3066 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 43605 <nil> <nil>}
I1220 08:16:14.004161    3066 main.go:141] libmachine: About to run SSH command:
ip addr show
I1220 08:16:14.079963    3066 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86387sec preferred_lft 86387sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 587sec preferred_lft 587sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:0d:9a:ba:1f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

I1220 08:16:14.080016    3066 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ec:5b:94 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86387sec preferred_lft 86387sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ce:e2:0a brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 587sec preferred_lft 587sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:0d:9a:ba:1f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

END SSH

I1220 08:16:14.080573    3066 ssh_runner.go:195] Run: cat /version.json
I1220 08:16:14.080584    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:16:14.080607    3066 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1220 08:16:14.080625    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:16:14.120129    3066 ssh_runner.go:195] Run: systemctl --version
I1220 08:16:14.123800    3066 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1220 08:16:14.374545    3066 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1220 08:16:14.374704    3066 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1220 08:16:14.417888    3066 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1220 08:16:14.417902    3066 start.go:472] detecting cgroup driver to use...
I1220 08:16:14.418075    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1220 08:16:14.435252    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1220 08:16:14.441175    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1220 08:16:14.446746    3066 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1220 08:16:14.446784    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1220 08:16:14.452283    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1220 08:16:14.457847    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1220 08:16:14.463403    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1220 08:16:14.468897    3066 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1220 08:16:14.474574    3066 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1220 08:16:14.480270    3066 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1220 08:16:14.485495    3066 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1220 08:16:14.490658    3066 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1220 08:16:14.574994    3066 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1220 08:16:14.585259    3066 start.go:472] detecting cgroup driver to use...
I1220 08:16:14.585298    3066 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1220 08:16:14.597344    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1220 08:16:14.610873    3066 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1220 08:16:14.624144    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1220 08:16:14.631787    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1220 08:16:14.639263    3066 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1220 08:16:14.654896    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1220 08:16:14.662630    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1220 08:16:14.673178    3066 ssh_runner.go:195] Run: which cri-dockerd
I1220 08:16:14.675313    3066 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1220 08:16:14.680395    3066 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1220 08:16:14.690396    3066 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1220 08:16:14.765538    3066 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1220 08:16:14.837999    3066 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1220 08:16:14.838113    3066 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1220 08:16:14.848011    3066 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1220 08:16:14.919274    3066 ssh_runner.go:195] Run: sudo systemctl restart docker
I1220 08:16:16.184316    3066 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.265022765s)
I1220 08:16:16.184391    3066 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1220 08:16:16.258889    3066 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1220 08:16:16.335807    3066 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1220 08:16:16.415460    3066 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1220 08:16:16.500521    3066 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1220 08:16:16.510281    3066 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1220 08:16:16.590487    3066 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1220 08:16:16.632092    3066 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1220 08:16:16.632169    3066 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1220 08:16:16.635954    3066 start.go:540] Will wait 60s for crictl version
I1220 08:16:16.635980    3066 ssh_runner.go:195] Run: which crictl
I1220 08:16:16.638382    3066 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1220 08:16:16.680278    3066 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1220 08:16:16.680340    3066 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1220 08:16:16.695153    3066 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1220 08:16:16.718217    3066 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1220 08:16:16.798703    3066 ssh_runner.go:195] Run: grep 192.168.59.1	host.minikube.internal$ /etc/hosts
I1220 08:16:16.800991    3066 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.59.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1220 08:16:16.807891    3066 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1220 08:16:16.807914    3066 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1220 08:16:16.825034    3066 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1220 08:16:16.825043    3066 docker.go:601] Images already preloaded, skipping extraction
I1220 08:16:16.825075    3066 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1220 08:16:16.836679    3066 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1220 08:16:16.836693    3066 cache_images.go:84] Images are preloaded, skipping loading
I1220 08:16:16.836729    3066 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1220 08:16:16.852296    3066 cni.go:84] Creating CNI manager for ""
I1220 08:16:16.852308    3066 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1220 08:16:16.852318    3066 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1220 08:16:16.852334    3066 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.59.100 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.59.100"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.59.100 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1220 08:16:16.852418    3066 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.59.100
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.59.100
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.59.100"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1220 08:16:16.852456    3066 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.59.100

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1220 08:16:16.852490    3066 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1220 08:16:16.866483    3066 binaries.go:44] Found k8s binaries, skipping transfer
I1220 08:16:16.866544    3066 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1220 08:16:16.872339    3066 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (371 bytes)
I1220 08:16:16.882358    3066 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1220 08:16:16.891918    3066 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2097 bytes)
I1220 08:16:16.902127    3066 ssh_runner.go:195] Run: grep 192.168.59.100	control-plane.minikube.internal$ /etc/hosts
I1220 08:16:16.904238    3066 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.59.100	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1220 08:16:16.910961    3066 certs.go:56] Setting up /home/truong/.minikube/profiles/minikube for IP: 192.168.59.100
I1220 08:16:16.910972    3066 certs.go:190] acquiring lock for shared ca certs: {Name:mkd9d63c6fab44e1511fbe9e8c48ce4ac8603e15 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1220 08:16:16.911197    3066 certs.go:199] skipping minikubeCA CA generation: /home/truong/.minikube/ca.key
I1220 08:16:16.911342    3066 certs.go:199] skipping proxyClientCA CA generation: /home/truong/.minikube/proxy-client-ca.key
I1220 08:16:16.911437    3066 certs.go:315] skipping minikube-user signed cert generation: /home/truong/.minikube/profiles/minikube/client.key
I1220 08:16:16.911519    3066 certs.go:315] skipping minikube signed cert generation: /home/truong/.minikube/profiles/minikube/apiserver.key.ee221796
I1220 08:16:16.911585    3066 certs.go:315] skipping aggregator signed cert generation: /home/truong/.minikube/profiles/minikube/proxy-client.key
I1220 08:16:16.911646    3066 certs.go:437] found cert: /home/truong/.minikube/certs/home/truong/.minikube/certs/ca-key.pem (1679 bytes)
I1220 08:16:16.911666    3066 certs.go:437] found cert: /home/truong/.minikube/certs/home/truong/.minikube/certs/ca.pem (1078 bytes)
I1220 08:16:16.911682    3066 certs.go:437] found cert: /home/truong/.minikube/certs/home/truong/.minikube/certs/cert.pem (1119 bytes)
I1220 08:16:16.911697    3066 certs.go:437] found cert: /home/truong/.minikube/certs/home/truong/.minikube/certs/key.pem (1675 bytes)
I1220 08:16:16.912090    3066 ssh_runner.go:362] scp /home/truong/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1220 08:16:16.926010    3066 ssh_runner.go:362] scp /home/truong/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1220 08:16:16.939374    3066 ssh_runner.go:362] scp /home/truong/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1220 08:16:16.952843    3066 ssh_runner.go:362] scp /home/truong/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1220 08:16:16.966951    3066 ssh_runner.go:362] scp /home/truong/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1220 08:16:16.980477    3066 ssh_runner.go:362] scp /home/truong/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1220 08:16:16.994134    3066 ssh_runner.go:362] scp /home/truong/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1220 08:16:17.007602    3066 ssh_runner.go:362] scp /home/truong/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1220 08:16:17.021419    3066 ssh_runner.go:362] scp /home/truong/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1220 08:16:17.035053    3066 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1220 08:16:17.044756    3066 ssh_runner.go:195] Run: openssl version
I1220 08:16:17.048080    3066 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1220 08:16:17.054341    3066 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1220 08:16:17.057125    3066 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Dec 19 03:19 /usr/share/ca-certificates/minikubeCA.pem
I1220 08:16:17.057152    3066 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1220 08:16:17.060462    3066 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1220 08:16:17.066631    3066 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1220 08:16:17.069020    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1220 08:16:17.072245    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1220 08:16:17.075566    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1220 08:16:17.078760    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1220 08:16:17.081993    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1220 08:16:17.085470    3066 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1220 08:16:17.088952    3066 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/truong:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1220 08:16:17.089009    3066 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1220 08:16:17.101476    3066 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1220 08:16:17.108605    3066 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1220 08:16:17.108612    3066 kubeadm.go:636] restartCluster start
I1220 08:16:17.108636    3066 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1220 08:16:17.115013    3066 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1220 08:16:17.115566    3066 kubeconfig.go:92] found "minikube" server: "https://192.168.59.100:8443"
I1220 08:16:17.118894    3066 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1220 08:16:17.124929    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:17.124953    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:17.133876    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:17.133881    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:17.133904    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:17.142422    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:17.642763    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:17.642857    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:17.683066    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:18.143406    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:18.143499    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:18.185970    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:18.643194    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:18.643289    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:18.684223    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:19.143431    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:19.143559    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:19.169424    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:19.642963    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:19.643078    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:19.684557    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:20.142863    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:20.142994    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:20.183241    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:20.643032    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:20.656059    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:20.671357    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:21.142910    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:21.143039    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:21.183132    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:21.642850    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:21.642959    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:21.682911    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:22.143095    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:22.143214    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:22.182532    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:22.642770    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:22.642902    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:22.682678    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:23.143084    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:23.143174    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:23.189070    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:23.643190    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:23.643284    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:23.682828    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:24.143074    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:24.143165    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:24.182682    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:24.643063    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:24.643155    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:24.683146    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:25.143279    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:25.143415    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:25.183222    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:25.643176    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:25.643270    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:25.686528    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:26.143000    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:26.143094    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:26.168418    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:26.642512    3066 api_server.go:166] Checking apiserver status ...
I1220 08:16:26.642603    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1220 08:16:26.669366    3066 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1220 08:16:27.125762    3066 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1220 08:16:27.125811    3066 kubeadm.go:1128] stopping kube-system containers ...
I1220 08:16:27.126016    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1220 08:16:27.175643    3066 docker.go:469] Stopping containers: [4b116e2bd3ad ad8050b05193 84c5a5781275 0c361525f50f 721527713b96 b76046062a7c 5e2e678084da 7ef7134642a8 18498700ecfc 276c2769c193 827e884a26c7 8066c3ea5c0a e46287398562 a5988db69f1f 8ab1ffa0cc53 8134ce9cd784 78ee6b6179c6 9c06b863f00b 7f3182c9086b 76d6bb057c9a 761bf3c0091a b23bdc467f87 474dce1c455b 248c704aa072 8b438f0c2c7a 2b603aed3d83 7298f11f6cbe]
I1220 08:16:27.175704    3066 ssh_runner.go:195] Run: docker stop 4b116e2bd3ad ad8050b05193 84c5a5781275 0c361525f50f 721527713b96 b76046062a7c 5e2e678084da 7ef7134642a8 18498700ecfc 276c2769c193 827e884a26c7 8066c3ea5c0a e46287398562 a5988db69f1f 8ab1ffa0cc53 8134ce9cd784 78ee6b6179c6 9c06b863f00b 7f3182c9086b 76d6bb057c9a 761bf3c0091a b23bdc467f87 474dce1c455b 248c704aa072 8b438f0c2c7a 2b603aed3d83 7298f11f6cbe
I1220 08:16:27.192025    3066 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1220 08:16:27.202486    3066 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1220 08:16:27.208741    3066 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1220 08:16:27.208785    3066 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1220 08:16:27.214482    3066 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1220 08:16:27.214505    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1220 08:16:27.316252    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1220 08:16:27.995320    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1220 08:16:28.129645    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1220 08:16:28.167330    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1220 08:16:28.211339    3066 api_server.go:52] waiting for apiserver process to appear ...
I1220 08:16:28.211395    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:28.221314    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:28.731573    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:29.231387    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:29.730865    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:30.230583    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:16:30.244032    3066 api_server.go:72] duration metric: took 2.032692555s to wait for apiserver process to appear ...
I1220 08:16:30.244044    3066 api_server.go:88] waiting for apiserver healthz status ...
I1220 08:16:30.244062    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:30.244444    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:30.244457    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:30.244713    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:30.744925    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:35.745436    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:16:35.745483    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:40.746609    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:16:40.746667    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:45.747956    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:16:45.748004    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:50.749003    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:16:50.749048    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:50.919929    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": read tcp 192.168.59.1:42118->192.168.59.100:8443: read: connection reset by peer
I1220 08:16:51.245234    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:51.246420    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:51.745122    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:51.746248    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:52.245378    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:52.246564    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:52.745162    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:52.746397    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:53.245846    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:53.247104    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:53.745141    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:53.746397    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:54.245736    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:54.246224    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:54.745780    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:54.746944    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:55.245519    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:55.246188    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:55.744821    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:55.745933    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:56.245848    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:56.247023    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:56.745577    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:56.746856    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:57.244882    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:57.245992    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:57.745759    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:57.746893    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:58.245555    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:58.246869    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:58.745699    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:58.746936    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:59.245502    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:59.246477    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:16:59.745265    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:16:59.746443    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:00.244942    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:00.246107    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:00.744900    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:00.746109    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:01.244907    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:01.246127    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:01.744895    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:01.746140    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:02.244800    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:02.246159    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:02.744811    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:02.745920    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:03.245776    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:03.246990    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:03.744813    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:03.746111    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:04.244831    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:04.246052    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:04.745798    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:04.746877    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:05.245752    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:05.246905    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:05.744817    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:05.746013    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:06.245716    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:06.246869    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:06.745479    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:06.746645    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:07.245782    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:07.246912    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:07.745716    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:07.746865    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:08.245739    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:08.246911    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:08.744939    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:13.745404    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:17:13.745448    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:18.746011    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:17:18.746088    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:23.747527    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:17:23.747573    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:28.748064    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:17:28.748109    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:28.989734    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": read tcp 192.168.59.1:44332->192.168.59.100:8443: read: connection reset by peer
I1220 08:17:29.245307    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:29.245958    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:29.745704    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:29.746883    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:30.245858    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:30.291820    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:30.291874    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:30.307865    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:30.307900    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:30.319402    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:30.319439    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:30.330155    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:30.330210    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:30.342820    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:30.342867    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:30.354655    3066 logs.go:284] 2 containers: [7969d342847b 91ccdfb8f460]
I1220 08:17:30.354732    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:30.365283    3066 logs.go:284] 0 containers: []
W1220 08:17:30.365292    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:30.365323    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:30.376546    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:30.376571    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:30.376580    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:30.387579    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:30.387586    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:30.499966    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:30.494048    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.494541    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.495893    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.496353    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.497686    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:30.494048    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.494541    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.495893    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.496353    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:30.497686    2704 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:30.499977    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:30.499987    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:30.513158    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:30.513169    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:30.559305    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:30.559321    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:30.572907    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:30.572917    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:30.585935    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:30.585947    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:30.599279    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:30.599294    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:30.610920    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:30.610926    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:30.610932    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:30.626846    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:30.626856    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:30.639753    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:30.639763    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:30.673220    3066 logs.go:123] Gathering logs for kube-controller-manager [91ccdfb8f460] ...
I1220 08:17:30.673231    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 91ccdfb8f460"
I1220 08:17:30.686325    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:30.686339    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:30.718692    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:30.718700    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:30.762512    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:30.762523    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:33.276994    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:33.278265    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:33.278398    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:33.316223    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:33.316280    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:33.330125    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:33.330177    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:33.341930    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:33.341988    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:33.362554    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:33.362617    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:33.386224    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:33.386301    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:33.397873    3066 logs.go:284] 2 containers: [7969d342847b 91ccdfb8f460]
I1220 08:17:33.397943    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:33.408966    3066 logs.go:284] 0 containers: []
W1220 08:17:33.408974    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:33.409002    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:33.419735    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:33.419754    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:33.419760    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:33.466885    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:33.466895    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:33.479640    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:33.479654    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:33.492388    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:33.492399    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:33.504546    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:33.504552    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:33.504575    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:33.515350    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:33.515358    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:33.565373    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:33.558873    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.559201    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.560480    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.560784    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.562357    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:33.558873    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.559201    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.560480    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.560784    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:33.562357    2890 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:33.565384    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:33.565392    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:33.600599    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:33.600609    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:33.621314    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:33.621326    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:33.665442    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:33.665454    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:33.681800    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:33.681811    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:33.695093    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:33.695103    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:33.707896    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:33.707907    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:33.740991    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:33.740998    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:33.755858    3066 logs.go:123] Gathering logs for kube-controller-manager [91ccdfb8f460] ...
I1220 08:17:33.755870    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 91ccdfb8f460"
I1220 08:17:36.269547    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:36.270666    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:36.270851    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:36.288705    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:36.288748    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:36.299881    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:36.299937    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:36.310551    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:36.310606    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:36.322065    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:36.322106    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:36.333527    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:36.333575    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:36.345120    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:36.345183    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:36.356027    3066 logs.go:284] 0 containers: []
W1220 08:17:36.356036    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:36.356097    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:36.367853    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:36.367901    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:36.367921    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:36.414687    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:36.414698    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:36.427531    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:36.427542    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:36.474991    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:36.475002    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:36.487800    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:36.487805    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:36.487817    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:36.504252    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:36.504262    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:36.517288    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:36.517298    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:36.529764    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:36.529777    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:36.542878    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:36.542888    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:36.594535    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:36.589351    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.589630    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.590946    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.591123    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.592363    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:36.589351    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.589630    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.590946    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.591123    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:36.592363    3059 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:36.594543    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:36.594551    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:36.609648    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:36.609658    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:36.622864    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:36.622875    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:36.633396    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:36.633404    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:36.668238    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:36.668248    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:39.204530    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:39.205598    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:39.205771    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:39.237097    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:39.237153    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:39.248867    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:39.248903    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:39.260817    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:39.260858    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:39.271764    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:39.271808    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:39.283053    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:39.283111    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:39.294698    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:39.294742    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:39.305670    3066 logs.go:284] 0 containers: []
W1220 08:17:39.305679    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:39.305708    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:39.316838    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:39.316858    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:39.316864    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:39.331061    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:39.331072    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:39.344170    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:39.344181    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:39.377105    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:39.377115    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:39.426876    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:39.426888    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:39.439958    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:39.439968    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:39.477785    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:39.477799    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:39.491633    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:39.491647    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:39.532401    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:39.532415    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:39.543249    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:39.543258    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:39.591275    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:39.586256    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.586597    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.587964    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.588164    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.589390    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:39.586256    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.586597    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.587964    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.588164    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:39.589390    3210 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:39.591293    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:39.591314    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:39.608100    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:39.608112    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:39.621570    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:39.621581    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:39.634846    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:39.634856    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:39.646958    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:42.147075    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:42.148219    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:42.148360    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:42.191016    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:42.191070    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:42.207599    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:42.207648    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:42.219223    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:42.219260    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:42.230705    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:42.230770    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:42.242181    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:42.242242    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:42.253461    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:42.253499    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:42.267690    3066 logs.go:284] 0 containers: []
W1220 08:17:42.267699    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:42.267741    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:42.279036    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:42.279053    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:42.279059    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:42.291780    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:42.291790    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:42.302522    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:42.302531    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:42.342715    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:42.342729    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:42.356063    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:42.356076    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:42.369030    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:42.369040    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:42.381380    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:42.381385    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:42.381392    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:42.397779    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:42.397799    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:42.446306    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:42.440948    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.441313    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.442776    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.442899    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.444478    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:42.440948    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.441313    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.442776    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.442899    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:42.444478    3329 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:42.446315    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:42.446322    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:42.460880    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:42.460890    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:42.496024    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:42.496034    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:42.510449    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:42.510459    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:42.523709    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:42.523720    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:42.557335    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:42.557344    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:45.105556    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:45.106684    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:45.106818    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:45.152335    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:45.152386    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:45.164495    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:45.164544    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:45.175904    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:45.175963    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:45.186809    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:45.186843    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:45.197979    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:45.198019    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:45.208929    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:45.208965    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:45.224478    3066 logs.go:284] 0 containers: []
W1220 08:17:45.224490    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:45.224547    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:45.236113    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:45.236131    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:45.236138    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:45.288131    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:45.283059    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.283444    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.284806    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.285065    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.286357    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:45.283059    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.283444    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.284806    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.285065    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:45.286357    3421 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:45.288153    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:45.288179    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:45.302044    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:45.302054    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:45.340806    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:45.340816    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:45.351412    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:45.351420    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:45.367827    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:45.367837    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:45.381130    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:45.381141    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:45.394006    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:45.394016    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:45.427844    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:45.427855    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:45.440776    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:45.440788    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:45.476571    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:45.476582    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:45.489511    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:45.489521    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:45.538779    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:45.538791    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:45.554433    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:45.554443    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:45.566802    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:48.067284    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:48.068409    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:48.068552    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:48.112116    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:48.112164    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:48.128305    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:48.128346    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:48.140215    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:48.140255    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:48.152083    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:48.152125    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:48.163289    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:48.163326    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:48.178898    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:48.178936    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:48.190052    3066 logs.go:284] 0 containers: []
W1220 08:17:48.190060    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:48.190101    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:48.201619    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:48.201646    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:48.201653    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:48.257142    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:48.251360    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.251540    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.253838    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.254093    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.255554    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:48.251360    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.251540    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.253838    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.254093    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:48.255554    3554 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:48.257151    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:48.257161    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:48.271811    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:48.271821    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:48.288015    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:48.288026    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:48.327682    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:48.327692    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:48.343124    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:48.343135    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:48.378809    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:48.378819    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:48.415976    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:48.415986    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:48.426919    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:48.426927    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:48.441252    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:48.441263    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:48.454318    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:48.454328    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:48.466670    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:48.466677    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:48.466684    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:48.516777    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:48.516786    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:48.529484    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:48.529494    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:51.043747    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:51.044207    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:51.044252    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:51.061874    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:51.061965    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:51.073194    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:51.073232    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:51.084046    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:51.084127    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:51.095108    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:51.095147    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:51.106263    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:51.106302    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:51.117438    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:51.117476    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:51.128114    3066 logs.go:284] 0 containers: []
W1220 08:17:51.128122    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:51.128155    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:51.140195    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:51.140233    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:51.140241    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:51.190524    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:51.190534    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:51.239606    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:51.235255    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.235415    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.236813    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.236930    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.238141    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:51.235255    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.235415    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.236813    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.236930    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:51.238141    3688 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:51.239613    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:51.239621    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:51.254591    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:51.254603    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:51.268192    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:51.268202    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:51.302764    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:51.302773    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:51.319541    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:51.319553    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:51.332391    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:51.332402    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:51.368393    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:51.368406    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:51.381704    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:51.381715    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:51.417944    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:51.417954    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:51.429386    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:51.429393    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:51.442804    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:51.442815    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:51.456108    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:51.456119    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:51.468874    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:53.969298    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:53.970401    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:53.970577    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:54.016236    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:54.016296    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:54.032470    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:54.032510    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:54.043838    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:54.043877    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:54.054936    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:54.054992    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:54.066856    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:54.066919    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:54.079572    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:54.079610    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:54.090466    3066 logs.go:284] 0 containers: []
W1220 08:17:54.090474    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:54.090502    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:54.101611    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:54.101636    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:54.101644    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:54.141945    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:54.141954    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:54.156541    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:54.156552    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:54.170121    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:54.170135    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:54.182895    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:54.182902    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:54.182910    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:54.196354    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:54.196364    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:54.244107    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:54.239611    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.239897    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.241106    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.241491    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.242639    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:54.239611    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.239897    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.241106    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.241491    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:54.242639    3861 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:54.244113    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:54.244121    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:54.259887    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:54.259897    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:54.271128    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:54.271138    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:54.287863    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:54.287874    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:54.324641    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:54.324653    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:54.338073    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:54.338084    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:54.351174    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:54.351185    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:54.384177    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:54.384189    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:56.937505    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:17:56.938728    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:17:56.938863    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:17:56.983431    3066 logs.go:284] 1 containers: [083a4820c889]
I1220 08:17:56.983486    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:17:57.000118    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:17:57.000161    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:17:57.011874    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:17:57.011927    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:17:57.029100    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:17:57.029137    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:17:57.039847    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:17:57.039886    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:17:57.051536    3066 logs.go:284] 1 containers: [7969d342847b]
I1220 08:17:57.051574    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:17:57.066812    3066 logs.go:284] 0 containers: []
W1220 08:17:57.066821    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:17:57.066851    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:17:57.078430    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:17:57.078448    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:17:57.078456    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:17:57.132812    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:17:57.132822    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:17:57.146591    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:17:57.146602    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:17:57.159165    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:17:57.159176    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:17:57.171534    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:17:57.171540    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:17:57.171548    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:17:57.182254    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:17:57.182263    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
I1220 08:17:57.201036    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:17:57.201047    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
I1220 08:17:57.214020    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:17:57.214030    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:17:57.249223    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:17:57.249231    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:17:57.286541    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:17:57.286556    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:17:57.300312    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:17:57.300323    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:17:57.349212    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:17:57.344987    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.345278    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.346516    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.346667    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.347869    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:17:57.344987    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.345278    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.346516    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.346667    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:17:57.347869    3992 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:17:57.349220    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:17:57.349227    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
I1220 08:17:57.366754    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:17:57.366767    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:17:57.400333    3066 logs.go:123] Gathering logs for container status ...
I1220 08:17:57.400346    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:17:59.979848    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:04.980847    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:18:04.981026    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:05.026084    3066 logs.go:284] 2 containers: [d964ef56fbb4 083a4820c889]
I1220 08:18:05.026138    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:05.041009    3066 logs.go:284] 1 containers: [2305d4a871c9]
I1220 08:18:05.041047    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:05.053034    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:05.053074    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:05.064234    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:05.064300    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:05.075639    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:05.075691    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:05.086494    3066 logs.go:284] 2 containers: [ac02f5480201 7969d342847b]
I1220 08:18:05.086529    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:05.096927    3066 logs.go:284] 0 containers: []
W1220 08:18:05.096936    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:05.096966    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:05.108024    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:05.108042    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:05.108048    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:05.121761    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:05.121773    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:05.134829    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:05.134840    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:05.169130    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:05.169140    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:05.179754    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:05.179761    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1220 08:18:20.833311    3066 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (15.653534006s)
W1220 08:18:20.833335    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:15.224212    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": net/http: TLS handshake timeout
E1220 01:18:20.828815    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:50702->127.0.0.1:8443: read: connection reset by peer
E1220 01:18:20.829137    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:20.830583    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:20.830924    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:15.224212    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": net/http: TLS handshake timeout
E1220 01:18:20.828815    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:50702->127.0.0.1:8443: read: connection reset by peer
E1220 01:18:20.829137    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:20.830583    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:20.830924    4258 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:20.833346    3066 logs.go:123] Gathering logs for etcd [2305d4a871c9] ...
I1220 08:18:20.833355    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2305d4a871c9"
W1220 08:18:20.847491    3066 logs.go:130] failed etcd [2305d4a871c9]: command: /bin/bash -c "docker logs --tail 400 2305d4a871c9" /bin/bash -c "docker logs --tail 400 2305d4a871c9": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 2305d4a871c9
 output: 
** stderr ** 
Error response from daemon: No such container: 2305d4a871c9

** /stderr **
I1220 08:18:20.847512    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:20.847519    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:20.868539    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:20.868551    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:20.882767    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:20.882774    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:20.882782    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:20.896133    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:20.896143    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:20.932493    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:20.932505    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:20.948886    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:20.948897    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:20.961630    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:20.961646    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:21.001331    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:21.001347    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:21.062113    3066 logs.go:123] Gathering logs for kube-apiserver [083a4820c889] ...
I1220 08:18:21.062124    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 083a4820c889"
W1220 08:18:21.073957    3066 logs.go:130] failed kube-apiserver [083a4820c889]: command: /bin/bash -c "docker logs --tail 400 083a4820c889" /bin/bash -c "docker logs --tail 400 083a4820c889": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 083a4820c889
 output: 
** stderr ** 
Error response from daemon: No such container: 083a4820c889

** /stderr **
I1220 08:18:21.073966    3066 logs.go:123] Gathering logs for kube-controller-manager [7969d342847b] ...
I1220 08:18:21.073993    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7969d342847b"
W1220 08:18:21.087218    3066 logs.go:130] failed kube-controller-manager [7969d342847b]: command: /bin/bash -c "docker logs --tail 400 7969d342847b" /bin/bash -c "docker logs --tail 400 7969d342847b": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 7969d342847b
 output: 
** stderr ** 
Error response from daemon: No such container: 7969d342847b

** /stderr **
I1220 08:18:23.588078    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:23.589166    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:23.589320    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:23.633596    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:23.633684    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:23.649492    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:23.649531    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:23.664948    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:23.665026    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:23.676886    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:23.676935    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:23.689035    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:23.689074    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:23.700838    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:23.700890    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:23.712599    3066 logs.go:284] 0 containers: []
W1220 08:18:23.712608    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:23.712654    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:23.725374    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:23.725399    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:23.725407    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:23.765022    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:23.765032    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:23.824920    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:23.824931    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:23.842814    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:23.842825    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:23.858418    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:23.858428    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:23.872026    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:23.872032    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:23.872040    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:23.886383    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:23.886392    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:23.900323    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:23.900333    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:23.915643    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:23.915654    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:23.926304    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:23.926314    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:23.977352    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:23.973200    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.973411    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.974939    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.975110    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.976435    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:23.973200    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.973411    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.974939    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.975110    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:23.976435    4638 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:23.977361    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:23.977372    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:24.018185    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:24.018211    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:24.032112    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:24.032123    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:24.045179    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:24.045190    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:26.580827    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:26.581924    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:26.582059    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:26.625941    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:26.626002    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:26.641670    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:26.641724    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:26.653238    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:26.653277    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:26.664206    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:26.664256    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:26.675375    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:26.675427    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:26.690576    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:26.690612    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:26.701243    3066 logs.go:284] 0 containers: []
W1220 08:18:26.701251    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:26.701281    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:26.712517    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:26.712536    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:26.712542    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:26.759302    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:26.755095    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.755312    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.757106    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.757228    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.758413    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:26.755095    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.755312    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.757106    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.757228    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:26.758413    4707 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:26.759311    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:26.759321    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:26.800882    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:26.800892    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:26.814475    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:26.814485    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:26.872167    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:26.872178    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:26.884898    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:26.884906    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:26.884914    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:26.899323    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:26.899335    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:26.917536    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:26.917547    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:26.955824    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:26.955835    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:26.966518    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:26.966527    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:26.978927    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:26.978937    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:26.992150    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:26.992160    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:27.005097    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:27.005107    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:27.040166    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:27.040179    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:29.557519    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:29.558703    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:29.558873    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:29.609599    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:29.609670    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:29.630372    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:29.630415    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:29.643463    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:29.643532    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:29.655490    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:29.655531    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:29.666917    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:29.666957    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:29.678557    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:29.678612    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:29.689352    3066 logs.go:284] 0 containers: []
W1220 08:18:29.689362    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:29.689390    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:29.700779    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:29.700796    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:29.700811    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:29.736127    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:29.736154    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:29.748976    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:29.748987    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:29.760145    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:29.760155    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:29.807801    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:29.803881    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.804518    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.805626    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.805781    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.806959    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:29.803881    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.804518    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.805626    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.805781    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:29.806959    4865 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:29.807807    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:29.807814    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:29.824363    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:29.824373    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:29.866228    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:29.866240    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:29.906547    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:29.906557    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:29.969981    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:29.969993    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:29.984196    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:29.984208    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:29.997673    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:29.997685    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:30.010217    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:30.010227    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:30.023181    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:30.023191    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:30.035867    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:30.035873    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:30.035882    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:32.551734    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:32.555726    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:32.555896    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:32.569978    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:32.570022    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:32.582250    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:32.582336    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:32.597608    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:32.597671    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:32.608916    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:32.608978    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:32.619883    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:32.619922    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:32.631423    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:32.631472    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:32.642510    3066 logs.go:284] 0 containers: []
W1220 08:18:32.642519    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:32.642561    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:32.653602    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:32.653622    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:32.653635    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:32.666225    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:32.666236    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:32.707286    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:32.707297    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:32.720345    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:32.720355    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:32.735268    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:32.735277    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:32.751470    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:32.751481    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:32.764687    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:32.764699    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:32.777145    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:32.777153    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:32.777159    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:32.815303    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:32.815314    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:32.874776    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:32.874788    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:32.912702    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:32.912713    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:32.926199    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:32.926210    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:32.971558    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:32.967491    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.967646    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.968991    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.969506    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.970838    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:32.967491    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.967646    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.968991    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.969506    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:32.970838    5041 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:32.971569    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:32.971579    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:32.984846    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:32.984856    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:35.497065    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:35.498152    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:35.498310    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:35.540673    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:35.540721    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:35.556074    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:35.556112    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:35.568009    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:35.568044    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:35.579765    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:35.579842    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:35.591474    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:35.591514    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:35.602792    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:35.602827    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:35.613449    3066 logs.go:284] 0 containers: []
W1220 08:18:35.613457    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:35.613488    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:35.625006    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:35.625024    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:35.625030    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:35.672828    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:35.667700    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.668086    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.669035    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.670381    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.670687    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:35.667700    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.668086    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.669035    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.670381    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:35.670687    5115 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:35.672836    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:35.672844    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:35.708918    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:35.708930    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:35.772077    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:35.772086    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:35.785496    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:35.785512    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:35.799478    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:35.799490    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:35.817383    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:35.817394    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:35.829084    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:35.829089    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:35.829096    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:35.864747    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:35.864756    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:35.880292    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:35.880302    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:35.920675    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:35.920707    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:35.931814    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:35.931821    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:35.952565    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:35.952574    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:35.965513    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:35.965523    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:38.479263    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:38.480432    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:38.480567    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:38.525679    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:38.525730    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:38.540456    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:38.540498    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:38.551859    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:38.551902    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:38.562980    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:38.563030    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:38.574251    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:38.574292    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:38.585306    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:38.585350    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:38.596252    3066 logs.go:284] 0 containers: []
W1220 08:18:38.596262    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:38.596303    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:38.608076    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:38.608097    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:38.608103    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:38.656688    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:38.652294    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.653314    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.654528    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.654678    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.655845    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:38.652294    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.653314    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.654528    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.654678    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:38.655845    5243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:38.656733    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:38.656756    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:38.673212    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:38.673221    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:38.715731    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:38.715742    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:38.729439    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:38.729449    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:38.742951    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:38.742961    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:38.755794    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:38.755805    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:38.817258    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:38.817269    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:38.828726    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:38.828734    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:38.842148    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:38.842158    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:38.854373    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:38.854379    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:38.854386    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:38.868767    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:38.868776    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:38.881186    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:38.881197    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:38.917166    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:38.917177    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:41.456968    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:41.458191    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:41.458340    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:41.499770    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:41.499820    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:41.515054    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:41.515100    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:41.527184    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:41.527254    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:41.538688    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:41.538756    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:41.549982    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:41.550034    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:41.561426    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:41.561463    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:41.572509    3066 logs.go:284] 0 containers: []
W1220 08:18:41.572518    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:41.572548    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:41.583910    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:41.583926    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:41.583932    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:41.597340    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:41.597352    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:41.611245    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:41.611256    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:41.655146    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:41.655156    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:41.720266    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:41.720277    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:41.733798    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:41.733809    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:41.774945    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:41.774955    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:41.785396    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:41.785404    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:41.801702    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:41.801712    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:41.814043    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:41.814054    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:41.857374    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:41.857385    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:41.869344    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:41.869351    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:41.869366    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:41.914195    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:41.910094    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.910507    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.911893    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.912095    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.913395    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:41.910094    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.910507    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.911893    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.912095    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:41.913395    5436 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:41.914203    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:41.914210    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:41.928776    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:41.928787    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:44.443766    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:44.444916    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:44.445052    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:44.490578    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:44.490637    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:44.507578    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:44.507620    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:44.519075    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:44.519114    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:44.530969    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:44.531025    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:44.542558    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:44.542600    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:44.554174    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:44.554224    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:44.565237    3066 logs.go:284] 0 containers: []
W1220 08:18:44.565247    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:44.565277    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:44.576282    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:44.576300    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:44.576306    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:44.590559    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:44.590570    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:44.604814    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:44.604824    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:44.617650    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:44.617661    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:44.657555    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:44.657566    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:44.722099    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:44.722114    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:44.735415    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:44.735435    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:44.746816    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:44.746824    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:44.791617    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:44.787960    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.788344    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.789608    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.789727    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.790894    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:44.787960    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.788344    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.789608    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.789727    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:44.790894    5563 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:44.791624    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:44.791631    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:44.804601    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:44.804611    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:44.847562    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:44.847573    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:44.860309    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:44.860316    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:44.860322    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:44.876169    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:44.876179    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:44.889622    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:44.889655    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:47.428661    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:47.429143    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:47.429193    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:47.444310    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:47.444362    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:47.455865    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:47.455920    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:47.467211    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:47.467315    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:47.478506    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:47.478575    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:47.489829    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:47.489895    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:47.500955    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:47.501024    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:47.511616    3066 logs.go:284] 0 containers: []
W1220 08:18:47.511623    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:47.511651    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:47.522544    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:47.522562    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:47.522568    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:47.566371    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:47.562702    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.563029    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.564320    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.564423    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.565642    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:47.562702    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.563029    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.564320    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.564423    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:47.565642    5648 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:47.566378    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:47.566385    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:47.580463    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:47.580474    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:47.592874    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:47.592885    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:47.603532    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:47.603540    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:47.621621    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:47.621637    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:47.636279    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:47.636289    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:47.672545    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:47.672556    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:47.740498    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:47.740508    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:47.754100    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:47.754111    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:47.791319    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:47.791328    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:47.803804    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:47.803814    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:47.844303    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:47.844314    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:47.861726    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:47.861738    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:47.874625    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:50.374759    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:50.375961    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:50.376107    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:50.421176    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:50.421233    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:50.435690    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:50.435738    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:50.448960    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:50.448999    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:50.460358    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:50.460408    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:50.471617    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:50.471672    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:50.482698    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:50.482751    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:50.493463    3066 logs.go:284] 0 containers: []
W1220 08:18:50.493472    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:50.493529    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:50.504724    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:50.504760    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:50.504767    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:50.518842    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:50.518852    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:50.560658    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:50.560670    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:50.574458    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:50.574470    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:50.640067    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:50.640076    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:50.683173    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:50.679686    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.680025    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.681335    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.681443    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.682649    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:50.679686    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.680025    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.681335    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.681443    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:50.682649    5814 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:50.683179    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:50.683186    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:50.695701    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:50.695713    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:50.708791    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:50.708801    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:50.720592    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:50.720598    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:50.720604    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:50.757298    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:50.757307    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:50.768471    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:50.768490    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:50.785511    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:50.785521    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:50.825138    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:50.825149    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:50.838608    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:50.838617    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:53.352187    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:53.353345    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:53.353482    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:53.400858    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:53.400907    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:53.414165    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:53.414221    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:53.425106    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:53.425146    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:53.436444    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:53.436497    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:53.447808    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:53.447865    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:53.458944    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:53.458982    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:53.472078    3066 logs.go:284] 0 containers: []
W1220 08:18:53.472101    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:53.472148    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:53.483221    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:53.483237    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:53.483247    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:53.494584    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:53.494594    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:53.506649    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:53.506655    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:53.506663    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:53.527492    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:53.527502    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:53.571649    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:53.571661    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:53.585740    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:53.585752    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:53.620906    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:53.620915    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:53.658537    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:53.658548    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:53.724218    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:53.724229    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:53.768902    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:53.765252    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.765564    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.766913    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.767109    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.768262    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:53.765252    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.765564    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.766913    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.767109    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:53.768262    5979 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:53.768916    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:53.768928    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:53.782786    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:53.782796    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:53.797386    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:53.797398    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:53.810102    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:53.810112    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:53.822925    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:53.822935    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:56.336038    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:56.337204    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:56.337344    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:56.381553    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:56.381609    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:56.395826    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:56.395864    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:56.407805    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:56.407844    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:56.418589    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:56.418654    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:56.429622    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:56.429685    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:56.441139    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:56.441177    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:56.452122    3066 logs.go:284] 0 containers: []
W1220 08:18:56.452145    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:56.452191    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:56.463817    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:56.463847    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:56.463853    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:56.477976    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:56.477986    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:56.521026    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:56.521038    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:56.535057    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:56.535068    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:56.548637    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:56.548647    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:56.565125    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:56.565136    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:18:56.577412    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:56.577423    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:56.614267    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:56.614279    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:56.626243    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:56.626256    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:56.676162    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:56.672597    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.672958    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.674016    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.674245    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.675616    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:56.672597    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.672958    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.674016    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.674245    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:56.675616    6105 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:56.676173    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:56.676181    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:56.714516    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:56.714541    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:56.781741    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:56.781750    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:56.794633    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:56.794645    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:56.807155    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:56.807161    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:56.807169    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:59.321086    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:18:59.322195    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:18:59.322331    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:18:59.383844    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:18:59.383903    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:18:59.404386    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:18:59.404426    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:18:59.415820    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:18:59.415885    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:18:59.427525    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:18:59.427575    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:18:59.438743    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:18:59.438803    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:18:59.449948    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:18:59.450001    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:18:59.460783    3066 logs.go:284] 0 containers: []
W1220 08:18:59.460809    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:18:59.460840    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:18:59.472644    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:18:59.472663    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:18:59.472668    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:18:59.483607    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:18:59.483615    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:18:59.529843    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:18:59.525563    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.525856    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.527216    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.527338    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.529113    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:18:59.525563    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.525856    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.527216    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.527338    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:18:59.529113    6217 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:18:59.529854    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:18:59.529878    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:18:59.544650    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:18:59.544660    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:18:59.588143    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:18:59.588153    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:18:59.601301    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:18:59.601312    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:18:59.614267    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:18:59.614278    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:18:59.652450    3066 logs.go:123] Gathering logs for container status ...
I1220 08:18:59.652461    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:18:59.693143    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:18:59.693153    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:18:59.705744    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:18:59.705755    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:18:59.717603    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:18:59.717615    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:18:59.717623    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:18:59.781315    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:18:59.781324    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:18:59.797805    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:18:59.797815    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:18:59.810564    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:18:59.810575    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:02.323605    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:02.324770    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:02.324910    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:02.373325    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:19:02.373378    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:02.388542    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:19:02.388621    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:02.399351    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:02.399400    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:02.410689    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:02.410727    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:02.422744    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:02.422784    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:02.433984    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:19:02.434058    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:02.445135    3066 logs.go:284] 0 containers: []
W1220 08:19:02.445146    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:02.445207    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:02.456520    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:02.456571    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:02.456577    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:02.526665    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:19:02.526674    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:19:02.543486    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:02.543497    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:02.555916    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:02.555926    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:02.600123    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:02.600134    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:02.616813    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:02.616825    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:02.630249    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:02.630260    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:02.645282    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:02.645293    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:02.659388    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:02.659399    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:02.671029    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:02.671038    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:02.671045    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:02.706253    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:02.706264    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:02.716619    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:02.716627    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:02.759331    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:02.755821    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.756280    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.757505    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.757609    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.758804    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:02.755821    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.756280    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.757505    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.757609    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:02.758804    6407 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:02.759337    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:19:02.759344    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:19:02.772424    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:02.772434    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:05.309058    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:05.310200    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:05.310344    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:05.327571    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:19:05.327624    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:05.339377    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:19:05.339414    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:05.350616    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:05.350654    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:05.362370    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:05.362408    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:05.373525    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:05.373565    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:05.384595    3066 logs.go:284] 1 containers: [ac02f5480201]
I1220 08:19:05.384631    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:05.395046    3066 logs.go:284] 0 containers: []
W1220 08:19:05.395068    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:05.395099    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:05.405614    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:05.405639    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:05.405645    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:05.471158    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:05.471171    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:05.482032    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:05.482039    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:05.529158    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:05.529167    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:05.543093    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:19:05.543103    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:19:05.556337    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:05.556348    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:05.592251    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:05.592262    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:05.637370    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:05.632633    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.632845    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.634063    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.634198    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.635333    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:05.632633    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.632845    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.634063    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.634198    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:05.635333    6520 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:05.637376    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:19:05.637383    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:19:05.654977    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:05.658912    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:05.671715    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:05.671727    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:05.683762    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:05.683769    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:05.683776    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:05.697726    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:05.697736    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:05.709760    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:05.709770    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:05.745801    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:05.745811    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:08.260035    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:08.261176    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:08.261312    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:08.278597    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:19:08.278634    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:08.289918    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:19:08.289965    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:08.301549    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:08.301586    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:08.318291    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:08.318329    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:08.331013    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:08.331057    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:08.342322    3066 logs.go:284] 2 containers: [0547b233025a ac02f5480201]
I1220 08:19:08.342374    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:08.353642    3066 logs.go:284] 0 containers: []
W1220 08:19:08.353667    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:08.353716    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:08.365313    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:08.365343    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:08.365349    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:08.431596    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:08.431605    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:08.442637    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:08.442646    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:08.456563    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:19:08.456582    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:19:08.469276    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:08.469286    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:08.482049    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:08.482060    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:08.527425    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:08.523664    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.524049    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.525049    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.525346    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.526750    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:08.523664    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.524049    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.525049    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.525346    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:08.526750    6695 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:08.527433    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:08.527454    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:08.542865    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:08.542874    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:08.555375    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:08.555382    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:08.555388    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:08.591298    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:08.591307    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:08.627998    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:08.628007    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:08.641565    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:19:08.641575    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:19:08.658070    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:08.658081    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:08.672498    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:08.672509    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:08.715028    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:08.715041    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:11.228976    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:11.230237    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:11.230380    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:11.247570    3066 logs.go:284] 1 containers: [d964ef56fbb4]
I1220 08:19:11.247625    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:11.258599    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:19:11.258635    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:11.270661    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:11.270700    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:11.282031    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:11.282067    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:11.292848    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:11.292888    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:11.303890    3066 logs.go:284] 2 containers: [0547b233025a ac02f5480201]
I1220 08:19:11.303984    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:11.314743    3066 logs.go:284] 0 containers: []
W1220 08:19:11.314750    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:11.314781    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:11.325717    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:11.325734    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:11.325740    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:11.368729    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:11.364783    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.364955    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.366232    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.366437    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.368137    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:11.364783    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.364955    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.366232    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.366437    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:11.368137    6835 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:11.368737    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:19:11.368744    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:19:11.385885    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:11.385894    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:11.398500    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:11.398510    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:11.410182    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:11.410188    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:11.410195    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:11.423219    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:11.423229    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:11.457491    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:11.457499    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:11.468382    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:11.468392    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:11.483661    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:11.483672    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:11.496866    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:11.496876    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:11.509209    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:11.509220    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:11.575744    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:11.575754    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:11.622085    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:11.622097    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:11.635121    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:19:11.635131    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:19:11.647971    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:11.647982    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:14.185718    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:19.187050    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1220 08:19:19.187196    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:19.223526    3066 logs.go:284] 2 containers: [7c80341e06ff d964ef56fbb4]
I1220 08:19:19.223570    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:19.236092    3066 logs.go:284] 1 containers: [847898ca0a0b]
I1220 08:19:19.236167    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:19.247411    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:19.247520    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:19.258907    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:19.258962    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:19.269944    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:19.269992    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:19.281266    3066 logs.go:284] 2 containers: [0547b233025a ac02f5480201]
I1220 08:19:19.281305    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:19.291859    3066 logs.go:284] 0 containers: []
W1220 08:19:19.291883    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:19.291912    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:19.303129    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:19.303147    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:19.303153    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:19.344121    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:19.344131    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:19.354955    3066 logs.go:123] Gathering logs for kube-controller-manager [ac02f5480201] ...
I1220 08:19:19.354963    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ac02f5480201"
I1220 08:19:19.367641    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:19.367652    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:19.387855    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:19.387864    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:19.425883    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:19.425896    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:19.441170    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:19.441181    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:19.454572    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:19.454583    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:19.467402    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:19.467413    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:19.481333    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:19.481344    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:19.525370    3066 logs.go:123] Gathering logs for kube-apiserver [d964ef56fbb4] ...
I1220 08:19:19.525382    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d964ef56fbb4"
I1220 08:19:19.542774    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:19.542784    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:19.555289    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:19.555300    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:19.567771    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:19.567782    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:19.579844    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:19.579850    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:19.579858    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:19.647716    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:19.647728    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1220 08:19:35.054734    3066 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (15.406994726s)
W1220 08:19:35.054752    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:29.689768    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": net/http: TLS handshake timeout
E1220 01:19:35.049790    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:52240->127.0.0.1:8443: read: connection reset by peer
E1220 01:19:35.051434    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:35.052722    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:35.052979    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:29.689768    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": net/http: TLS handshake timeout
E1220 01:19:35.049790    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:52240->127.0.0.1:8443: read: connection reset by peer
E1220 01:19:35.051434    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:35.052722    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:35.052979    7145 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:37.555029    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:37.555458    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:37.555500    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:37.567061    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:37.567111    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:37.578158    3066 logs.go:284] 2 containers: [e19f6e6d6f13 847898ca0a0b]
I1220 08:19:37.578196    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:37.589126    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:37.589166    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:37.606408    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:37.606490    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:37.618157    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:37.618196    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:37.630139    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:37.630243    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:37.641267    3066 logs.go:284] 0 containers: []
W1220 08:19:37.641277    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:37.641309    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:37.652367    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:37.652417    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:37.652424    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:37.665085    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:37.665096    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:37.736531    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:37.736544    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:37.752007    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:37.752019    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:37.765083    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:37.765095    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:37.812310    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:37.808740    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.809153    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.810405    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.810704    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.811939    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:37.808740    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.809153    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.810405    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.810704    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:37.811939    7394 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:37.812317    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:37.812324    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:37.826353    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:37.826364    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:37.849307    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:37.849313    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:37.849320    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:37.889375    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:37.889383    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:37.900534    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:37.900541    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:37.920439    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:37.920450    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:37.934148    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:37.934159    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:37.969866    3066 logs.go:123] Gathering logs for etcd [847898ca0a0b] ...
I1220 08:19:37.969877    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 847898ca0a0b"
I1220 08:19:37.985073    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:37.985082    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:37.997707    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:37.997718    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:40.545000    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:40.546212    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:40.546362    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:40.591099    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:40.591154    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:40.608417    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:40.608454    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:40.619328    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:40.619395    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:40.630456    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:40.630506    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:40.642369    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:40.651281    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:40.663076    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:40.663117    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:40.674589    3066 logs.go:284] 0 containers: []
W1220 08:19:40.674596    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:40.674628    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:40.685480    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:40.685497    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:40.685503    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:40.754716    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:40.754731    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:40.766113    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:40.766124    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:40.779028    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:40.779040    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:40.816376    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:40.816386    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:40.860613    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:40.857073    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.857449    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.858834    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.859024    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.860085    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:40.857073    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.857449    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.858834    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.859024    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:40.860085    7542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:40.860620    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:40.860628    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:40.875617    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:40.875631    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:40.922562    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:40.922571    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:40.936442    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:40.936452    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:40.949712    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:40.949736    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:40.963353    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:40.963364    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:40.976268    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:40.976278    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:40.987807    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:40.987814    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:40.987822    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:41.024311    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:41.024326    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:43.546630    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:43.547799    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:43.547940    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:43.593888    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:43.593944    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:43.611301    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:43.611345    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:43.622195    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:43.622268    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:43.633386    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:43.633436    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:43.644545    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:43.644604    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:43.655733    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:43.655819    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:43.666597    3066 logs.go:284] 0 containers: []
W1220 08:19:43.666607    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:43.666670    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:43.677948    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:43.677987    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:43.677995    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:43.691265    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:43.691276    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:43.739043    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:43.734525    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.734923    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.736347    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.736471    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.738339    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:43.734525    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.734923    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.736347    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.736471    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:43.738339    7684 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:43.739051    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:43.739060    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:43.753782    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:43.753793    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:43.766889    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:43.766899    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:43.812435    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:43.812445    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:43.823527    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:43.823536    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:43.839979    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:43.839990    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:43.854067    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:43.854077    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:43.893216    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:43.893225    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:43.906391    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:43.906402    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:43.919338    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:43.919347    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:43.919354    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:43.966093    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:43.966105    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:44.035116    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:44.035126    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:46.549795    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:46.550831    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:46.550973    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:46.597398    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:46.597463    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:46.611920    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:46.611955    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:46.622829    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:46.622886    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:46.634239    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:46.634290    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:46.645135    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:46.645177    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:46.660853    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:46.660910    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:46.672526    3066 logs.go:284] 0 containers: []
W1220 08:19:46.672536    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:46.672585    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:46.683641    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:46.683659    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:46.683665    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:46.696255    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:46.696265    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:46.765723    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:46.765735    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:46.809267    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:46.805632    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.806425    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.807279    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.807882    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.808986    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:46.805632    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.806425    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.807279    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.807882    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:46.808986    7819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:46.809276    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:46.809284    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:46.823117    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:46.823128    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:46.835380    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:46.835390    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:46.884891    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:46.884902    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:46.897973    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:46.897983    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:46.933997    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:46.934006    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:46.944979    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:46.944987    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:46.966647    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:46.966657    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:46.980286    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:46.980296    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:46.993119    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:46.993130    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:47.005053    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:47.005059    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:47.005066    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:49.541421    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:49.542526    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:49.542702    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:49.590001    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:49.590056    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:49.604899    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:49.604971    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:49.615879    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:49.615961    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:49.626863    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:49.626915    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:49.637846    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:49.637899    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:49.649057    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:49.649097    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:49.659504    3066 logs.go:284] 0 containers: []
W1220 08:19:49.659512    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:49.659556    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:49.670575    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:49.670594    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:49.670614    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:49.684818    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:49.684829    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:49.730383    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:49.730393    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:49.743641    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:49.743651    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:49.755562    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:49.755568    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:49.755575    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:49.791835    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:49.791846    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:49.804693    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:49.804704    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:49.817086    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:49.817097    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:49.828065    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:49.828073    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:49.870362    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:49.867040    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.867414    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.868643    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.868746    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.869927    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:49.867040    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.867414    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.868643    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.868746    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:49.869927    8004 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:49.870373    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:49.870397    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:49.886925    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:49.886936    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:49.900721    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:49.900731    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:49.913319    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:49.913330    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:49.952069    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:49.952078    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:52.522083    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:52.523067    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:52.523187    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:52.569356    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:52.569438    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:52.583864    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:52.583900    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:52.594911    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:52.594985    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:52.606773    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:52.606852    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:52.617577    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:52.617620    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:52.628319    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:52.628366    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:52.638817    3066 logs.go:284] 0 containers: []
W1220 08:19:52.638825    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:52.638855    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:52.649919    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:52.649938    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:52.649958    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:52.660713    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:52.660720    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:52.676103    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:52.676113    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:52.692205    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:52.692216    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:52.705264    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:52.705275    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:52.717779    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:52.717790    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:52.783830    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:52.783838    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:52.829812    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:52.829823    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:52.843132    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:52.843145    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:52.879535    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:52.879544    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:52.892435    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:52.892445    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:52.905674    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:52.905685    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:52.917509    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:52.917515    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:52.917521    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:52.957256    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:52.957267    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:53.001404    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:52.997521    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.997851    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.999176    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.999465    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:53.000670    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:52.997521    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.997851    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.999176    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:52.999465    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:53.000670    8149 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:55.502482    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:55.503683    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:55.503830    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:55.545283    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:55.545336    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:55.559018    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:55.559072    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:55.569920    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:55.569959    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:55.581641    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:55.581695    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:55.593018    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:55.593059    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:55.603963    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:55.604004    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:55.619253    3066 logs.go:284] 0 containers: []
W1220 08:19:55.619264    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:55.619308    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:55.630181    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:55.630204    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:55.630224    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:55.701935    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:55.701947    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:55.716899    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:55.716911    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:55.730246    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:55.730257    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:55.742169    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:55.742178    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:55.742186    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:55.752974    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:55.752981    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:55.794996    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:55.791485    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.792203    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.793256    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.793449    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.794646    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:55.791485    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.792203    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.793256    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.793449    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:55.794646    8243 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:55.795003    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:55.795010    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:55.811360    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:55.811370    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:19:55.824650    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:55.824661    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:55.838246    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:55.838257    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:55.873314    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:55.873324    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:55.885723    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:55.885733    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:55.932462    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:55.932474    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:55.945722    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:55.945733    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:58.483688    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:19:58.484768    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:19:58.484916    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:19:58.529660    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:19:58.529718    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:19:58.546334    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:19:58.546380    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:19:58.557884    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:19:58.557924    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:19:58.569446    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:19:58.569500    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:19:58.580753    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:19:58.580794    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:19:58.592118    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:19:58.592153    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:19:58.602460    3066 logs.go:284] 0 containers: []
W1220 08:19:58.602471    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:19:58.602501    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:19:58.613426    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:19:58.613445    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:19:58.613452    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:19:58.628238    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:19:58.628249    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:19:58.641775    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:19:58.641786    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:19:58.654189    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:19:58.654197    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:19:58.654207    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:19:58.691607    3066 logs.go:123] Gathering logs for container status ...
I1220 08:19:58.691615    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:19:58.733592    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:19:58.733611    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:19:58.744813    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:19:58.744820    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:19:58.789847    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:19:58.786462    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.786824    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.788167    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.788416    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.789427    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:19:58.786462    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.786824    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.788167    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.788416    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:19:58.789427    8393 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:19:58.789852    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:19:58.789862    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:19:58.802338    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:19:58.802349    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:19:58.815108    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:19:58.815123    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:19:58.828437    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:19:58.828447    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:19:58.895243    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:19:58.895252    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:19:58.911630    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:19:58.911641    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:19:58.959269    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:19:58.959280    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:01.472870    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:01.474017    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:01.474155    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:01.519703    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:01.519756    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:01.531425    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:01.531501    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:01.542947    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:01.543004    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:01.554445    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:01.554487    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:01.565353    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:01.565396    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:01.576642    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:01.576696    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:01.587719    3066 logs.go:284] 0 containers: []
W1220 08:20:01.587728    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:01.587761    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:01.598937    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:01.598956    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:01.598963    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:01.614391    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:01.614404    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:01.627633    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:01.627643    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:01.674663    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:01.671153    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.671515    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.672769    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.673030    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.674250    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:01.671153    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.671515    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.672769    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.673030    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:01.674250    8494 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:01.674671    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:01.674680    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:01.691480    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:01.691490    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:01.706279    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:01.706290    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:01.718800    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:01.718810    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:01.757591    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:01.757602    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:01.795120    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:01.795131    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:01.864483    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:01.864496    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:01.875905    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:01.875912    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:01.923197    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:01.923207    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:01.937963    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:01.937976    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:01.951013    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:01.951025    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:01.963690    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:04.464254    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:04.465425    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:04.465567    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:04.509853    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:04.509911    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:04.524223    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:04.524258    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:04.535478    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:04.535561    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:04.546973    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:04.547065    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:04.558721    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:04.558783    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:04.569807    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:04.569854    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:04.580146    3066 logs.go:284] 0 containers: []
W1220 08:20:04.580171    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:04.580226    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:04.590966    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:04.590988    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:04.590996    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:04.630257    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:04.630265    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:04.679043    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:04.679054    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:04.691657    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:04.691668    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:04.704769    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:04.704779    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:04.716538    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:04.716545    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:04.716553    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:04.765123    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:04.765135    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:04.778680    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:04.778691    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:04.790498    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:04.790505    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:04.810258    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:04.810268    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:04.824677    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:04.824687    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:04.894826    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:04.894841    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:04.938979    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:04.935572    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.935777    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.936984    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.937107    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.938381    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:04.935572    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.935777    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.936984    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.937107    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:04.938381    8692 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:04.938988    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:04.938995    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:04.953937    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:04.953949    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:07.467676    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:07.468891    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:07.469047    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:07.513688    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:07.513764    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:07.528387    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:07.528424    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:07.543772    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:07.543824    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:07.554991    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:07.555052    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:07.566030    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:07.566105    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:07.577370    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:07.577409    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:07.587774    3066 logs.go:284] 0 containers: []
W1220 08:20:07.587782    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:07.587814    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:07.598845    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:07.598864    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:07.598872    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:07.609784    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:07.609792    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:07.625390    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:07.625402    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:07.681946    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:07.681956    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:07.698712    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:07.698724    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:07.747646    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:07.747656    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:07.761022    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:07.761033    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:07.772897    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:07.772906    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:07.772913    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:07.840059    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:07.840067    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:07.884369    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:07.880919    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.881313    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.882662    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.882857    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.884014    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:07.880919    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.881313    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.882662    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.882857    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:07.884014    8809 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:07.884378    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:07.884385    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:07.899269    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:07.899279    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:07.911177    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:07.911188    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:07.924426    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:07.924436    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:07.936821    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:07.936832    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:10.476243    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:10.477245    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:10.477388    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:10.522818    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:10.522878    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:10.539550    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:10.539593    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:10.550521    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:10.550563    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:10.560900    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:10.560935    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:10.571521    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:10.571559    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:10.582386    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:10.582455    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:10.594113    3066 logs.go:284] 0 containers: []
W1220 08:20:10.594123    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:10.594153    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:10.605979    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:10.605998    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:10.606005    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:10.617873    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:10.617879    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:10.617887    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:10.653806    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:10.653817    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:10.723563    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:10.723574    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:10.736475    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:10.736486    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:10.751861    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:10.751872    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:10.788881    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:10.788892    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:10.803904    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:10.803915    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:10.817023    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:10.817060    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:10.833187    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:10.833197    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:10.884295    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:10.884306    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:10.897935    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:10.897945    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:10.908784    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:10.908792    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:10.955540    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:10.952020    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.952342    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.953593    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.953778    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.955029    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:10.952020    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.952342    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.953593    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.953778    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:10.955029    8970 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:10.955549    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:10.955560    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:13.472890    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:13.474005    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:13.474141    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:13.522850    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:13.522926    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:13.538888    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:13.538939    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:13.549888    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:13.549963    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:13.560964    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:13.561014    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:13.571998    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:13.572057    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:13.583240    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:13.583279    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:13.593970    3066 logs.go:284] 0 containers: []
W1220 08:20:13.593982    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:13.594027    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:13.605390    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:13.605447    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:13.605456    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:13.616698    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:13.616706    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:13.631140    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:13.631150    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:13.647695    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:13.647701    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:13.647724    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:13.664057    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:13.664067    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:13.705278    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:13.705291    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:13.748505    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:13.744948    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.745293    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.746649    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.746906    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.748128    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:13.744948    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.745293    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.746649    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.746906    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:13.748128    9066 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:13.748512    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:13.748521    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:13.797044    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:13.797055    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:13.817766    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:13.817777    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:13.830819    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:13.830829    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:13.843906    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:13.843917    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:13.882006    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:13.882015    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:13.951163    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:13.951175    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:13.964051    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:13.964075    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:16.477553    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:16.478646    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:16.478787    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:16.494438    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:16.494509    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:16.505699    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:16.505748    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:16.516974    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:16.517024    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:16.528890    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:16.528929    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:16.540114    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:16.540170    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:16.550832    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:16.550873    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:16.561002    3066 logs.go:284] 0 containers: []
W1220 08:20:16.561010    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:16.561037    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:16.572240    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:16.572258    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:16.572264    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:16.583676    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:16.583683    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:16.596164    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:16.596174    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:16.608758    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:16.608768    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:16.634022    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:16.634032    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:16.648034    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:16.648045    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:16.660089    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:16.660095    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:16.660102    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:16.674302    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:16.674313    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:16.687911    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:16.687920    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:16.725750    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:16.725763    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:16.793426    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:16.793435    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:16.840853    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:16.837651    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.837876    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.839129    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.839280    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.840421    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:16.837651    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.837876    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.839129    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.839280    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:16.840421    9225 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:16.840863    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:16.840874    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:16.894383    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:16.894393    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:16.907265    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:16.907275    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:19.444923    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:19.446025    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:19.446163    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:19.479081    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:19.479174    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:19.491866    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:19.491929    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:19.502915    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:19.502969    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:19.514015    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:19.514084    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:19.524829    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:19.524920    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:19.536274    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:19.536318    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:19.547229    3066 logs.go:284] 0 containers: []
W1220 08:20:19.547239    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:19.547288    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:19.558636    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:19.558655    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:19.558661    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:19.594548    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:19.594560    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:19.610918    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:19.610927    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:19.623341    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:19.623351    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:19.672127    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:19.672137    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:19.685493    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:19.685503    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:19.699657    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:19.699670    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:19.769210    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:19.769221    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:19.780547    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:19.780556    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:19.793988    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:19.793998    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:19.809433    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:19.809443    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:19.822594    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:19.822605    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:19.835054    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:19.835060    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:19.835068    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:19.881519    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:19.874875    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.875277    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.876595    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.876702    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.877894    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:19.874875    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.875277    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.876595    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.876702    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:19.877894    9377 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:19.881526    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:19.881533    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:22.419191    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:22.420357    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:22.420501    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:22.462171    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:22.462237    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:22.476499    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:22.476568    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:22.487517    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:22.487555    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:22.498612    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:22.498678    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:22.509487    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:22.509525    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:22.520601    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:22.520684    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:22.531268    3066 logs.go:284] 0 containers: []
W1220 08:20:22.531277    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:22.531311    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:22.542928    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:22.542945    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:22.542953    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:22.555340    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:22.555346    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:22.555353    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:22.570101    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:22.570111    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:22.583107    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:22.583118    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:22.595923    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:22.595934    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:22.608804    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:22.608814    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:22.621310    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:22.621320    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:22.690774    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:22.690785    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:22.708262    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:22.708273    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:22.722483    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:22.722493    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:22.759466    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:22.759477    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:22.800013    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:22.800023    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:22.811223    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:22.811231    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:22.853673    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:22.850425    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.850833    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.852039    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.852165    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.853351    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:22.850425    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.850833    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.852039    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.852165    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:22.853351    9503 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:22.853679    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:22.853687    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:25.403023    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:25.404221    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:25.404371    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:25.422163    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:25.422240    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:25.433009    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:25.433055    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:25.444559    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:25.444615    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:25.456698    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:25.456765    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:25.467732    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:25.467802    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:25.478867    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:25.478932    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:25.489944    3066 logs.go:284] 0 containers: []
W1220 08:20:25.489952    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:25.489996    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:25.501158    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:25.501174    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:25.501180    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:25.512123    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:25.512131    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:25.549184    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:25.549191    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:25.588056    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:25.588067    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:25.605210    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:25.605221    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:25.654849    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:25.654859    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:25.669011    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:25.669027    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:25.682430    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:25.682442    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:25.694667    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:25.694686    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:25.694694    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:25.768245    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:25.768255    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:25.812327    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:25.808838    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.809182    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.810538    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.810801    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.811990    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:25.808838    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.809182    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.810538    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.810801    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:25.811990    9629 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:25.812336    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:25.812345    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:25.827434    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:25.827444    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:25.840081    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:25.840092    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:25.853073    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:25.853086    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:28.366803    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:28.367912    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:28.368056    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1220 08:20:28.416481    3066 logs.go:284] 1 containers: [7c80341e06ff]
I1220 08:20:28.416526    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1220 08:20:28.429792    3066 logs.go:284] 1 containers: [e19f6e6d6f13]
I1220 08:20:28.429829    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1220 08:20:28.440839    3066 logs.go:284] 1 containers: [ad8050b05193]
I1220 08:20:28.440874    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1220 08:20:28.451767    3066 logs.go:284] 2 containers: [641a42a54143 7ef7134642a8]
I1220 08:20:28.451803    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1220 08:20:28.462818    3066 logs.go:284] 1 containers: [721527713b96]
I1220 08:20:28.462896    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1220 08:20:28.474078    3066 logs.go:284] 1 containers: [0547b233025a]
I1220 08:20:28.474117    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1220 08:20:28.485277    3066 logs.go:284] 0 containers: []
W1220 08:20:28.485286    3066 logs.go:286] No container was found matching "kindnet"
I1220 08:20:28.485317    3066 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1220 08:20:28.496952    3066 logs.go:284] 2 containers: [4b116e2bd3ad 0c361525f50f]
I1220 08:20:28.496970    3066 logs.go:123] Gathering logs for kube-proxy [721527713b96] ...
I1220 08:20:28.496977    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 721527713b96"
I1220 08:20:28.510135    3066 logs.go:123] Gathering logs for kube-controller-manager [0547b233025a] ...
I1220 08:20:28.510147    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0547b233025a"
I1220 08:20:28.523036    3066 logs.go:123] Gathering logs for dmesg ...
I1220 08:20:28.523047    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1220 08:20:28.534364    3066 logs.go:123] Gathering logs for kube-apiserver [7c80341e06ff] ...
I1220 08:20:28.534373    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c80341e06ff"
I1220 08:20:28.558827    3066 logs.go:123] Gathering logs for kube-scheduler [641a42a54143] ...
I1220 08:20:28.558836    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 641a42a54143"
I1220 08:20:28.608735    3066 logs.go:123] Gathering logs for kube-scheduler [7ef7134642a8] ...
I1220 08:20:28.608746    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7ef7134642a8"
I1220 08:20:28.622452    3066 logs.go:123] Gathering logs for storage-provisioner [0c361525f50f] ...
I1220 08:20:28.622463    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0c361525f50f"
W1220 08:20:28.634751    3066 logs.go:130] failed storage-provisioner [0c361525f50f]: command: /bin/bash -c "docker logs --tail 400 0c361525f50f" /bin/bash -c "docker logs --tail 400 0c361525f50f": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1220 08:20:28.634771    3066 logs.go:123] Gathering logs for Docker ...
I1220 08:20:28.634793    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1220 08:20:28.673834    3066 logs.go:123] Gathering logs for kubelet ...
I1220 08:20:28.673843    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1220 08:20:28.741601    3066 logs.go:123] Gathering logs for describe nodes ...
I1220 08:20:28.741613    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1220 08:20:28.784788    3066 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1220 01:20:28.781355    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.781690    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.783059    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.783279    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.784488    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1220 01:20:28.781355    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.781690    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.783059    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.783279    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E1220 01:20:28.784488    9744 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1220 08:20:28.784795    3066 logs.go:123] Gathering logs for container status ...
I1220 08:20:28.784803    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1220 08:20:28.822647    3066 logs.go:123] Gathering logs for etcd [e19f6e6d6f13] ...
I1220 08:20:28.822657    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e19f6e6d6f13"
I1220 08:20:28.837171    3066 logs.go:123] Gathering logs for coredns [ad8050b05193] ...
I1220 08:20:28.837181    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad8050b05193"
I1220 08:20:28.849620    3066 logs.go:123] Gathering logs for storage-provisioner [4b116e2bd3ad] ...
I1220 08:20:28.849636    3066 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4b116e2bd3ad"
I1220 08:20:31.362387    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:31.363516    3066 api_server.go:269] stopped: https://192.168.59.100:8443/healthz: Get "https://192.168.59.100:8443/healthz": dial tcp 192.168.59.100:8443: connect: connection refused
I1220 08:20:31.363599    3066 kubeadm.go:640] restartCluster took 4m14.25497758s
W1220 08:20:31.363703    3066 out.go:239] ü§¶  Unable to restart cluster, will reset it: apiserver health: apiserver healthz never reported healthy: context deadline exceeded
I1220 08:20:31.363786    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I1220 08:20:34.601571    3066 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (3.237771029s)
I1220 08:20:34.601604    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1220 08:20:34.609706    3066 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1220 08:20:34.615223    3066 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1220 08:20:34.620504    3066 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1220 08:20:34.620520    3066 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1220 08:20:34.649347    3066 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I1220 08:20:34.649381    3066 kubeadm.go:322] [preflight] Running pre-flight checks
I1220 08:20:34.740866    3066 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I1220 08:20:34.740948    3066 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1220 08:20:34.741035    3066 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1220 08:20:34.925341    3066 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1220 08:20:34.935156    3066 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1220 08:20:34.935229    3066 kubeadm.go:322] [certs] Using existing ca certificate authority
I1220 08:20:34.935273    3066 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I1220 08:20:34.935324    3066 kubeadm.go:322] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I1220 08:20:34.935361    3066 kubeadm.go:322] [certs] Using existing front-proxy-ca certificate authority
I1220 08:20:34.935405    3066 kubeadm.go:322] [certs] Using existing front-proxy-client certificate and key on disk
I1220 08:20:34.935438    3066 kubeadm.go:322] [certs] Using existing etcd/ca certificate authority
I1220 08:20:34.935475    3066 kubeadm.go:322] [certs] Using existing etcd/server certificate and key on disk
I1220 08:20:34.935520    3066 kubeadm.go:322] [certs] Using existing etcd/peer certificate and key on disk
I1220 08:20:34.935568    3066 kubeadm.go:322] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I1220 08:20:34.935615    3066 kubeadm.go:322] [certs] Using existing apiserver-etcd-client certificate and key on disk
I1220 08:20:34.935638    3066 kubeadm.go:322] [certs] Using the existing "sa" key
I1220 08:20:34.935671    3066 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1220 08:20:35.111417    3066 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I1220 08:20:35.316877    3066 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1220 08:20:35.549212    3066 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1220 08:20:35.627652    3066 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1220 08:20:35.628017    3066 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1220 08:20:35.629705    3066 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1220 08:20:35.639618    3066 out.go:204]     ‚ñ™ Booting up control plane ...
I1220 08:20:35.639766    3066 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1220 08:20:35.639881    3066 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1220 08:20:35.639949    3066 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1220 08:20:35.641507    3066 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1220 08:20:35.642164    3066 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1220 08:20:35.642201    3066 kubeadm.go:322] [kubelet-start] Starting the kubelet
I1220 08:20:35.727166    3066 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1220 08:20:41.233596    3066 kubeadm.go:322] [apiclient] All control plane components are healthy after 5.505372 seconds
I1220 08:20:41.233976    3066 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1220 08:20:41.269526    3066 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1220 08:20:41.821336    3066 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I1220 08:20:41.821552    3066 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1220 08:20:42.330342    3066 kubeadm.go:322] [bootstrap-token] Using token: n9uqvd.ub3ybjnd4zo47yss
I1220 08:20:42.339299    3066 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1220 08:20:42.339409    3066 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1220 08:20:42.340842    3066 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1220 08:20:42.344586    3066 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1220 08:20:42.346465    3066 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1220 08:20:42.348202    3066 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1220 08:20:42.349752    3066 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1220 08:20:42.362495    3066 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1220 08:20:42.536037    3066 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I1220 08:20:42.743845    3066 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I1220 08:20:42.744593    3066 kubeadm.go:322] 
I1220 08:20:42.744675    3066 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I1220 08:20:42.744680    3066 kubeadm.go:322] 
I1220 08:20:42.744737    3066 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I1220 08:20:42.744740    3066 kubeadm.go:322] 
I1220 08:20:42.744756    3066 kubeadm.go:322]   mkdir -p $HOME/.kube
I1220 08:20:42.744800    3066 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1220 08:20:42.744850    3066 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1220 08:20:42.744857    3066 kubeadm.go:322] 
I1220 08:20:42.744918    3066 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I1220 08:20:42.744923    3066 kubeadm.go:322] 
I1220 08:20:42.744972    3066 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1220 08:20:42.744975    3066 kubeadm.go:322] 
I1220 08:20:42.745021    3066 kubeadm.go:322] You should now deploy a pod network to the cluster.
I1220 08:20:42.745100    3066 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1220 08:20:42.745179    3066 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1220 08:20:42.745183    3066 kubeadm.go:322] 
I1220 08:20:42.745254    3066 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I1220 08:20:42.745322    3066 kubeadm.go:322] and service account keys on each node and then running the following as root:
I1220 08:20:42.745325    3066 kubeadm.go:322] 
I1220 08:20:42.745423    3066 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token n9uqvd.ub3ybjnd4zo47yss \
I1220 08:20:42.745509    3066 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:b1099a0a36ba8ecc0c1ce945f60592a322801a8743b416d6f0973f0a83fa88c9 \
I1220 08:20:42.745527    3066 kubeadm.go:322] 	--control-plane 
I1220 08:20:42.745530    3066 kubeadm.go:322] 
I1220 08:20:42.745592    3066 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I1220 08:20:42.745595    3066 kubeadm.go:322] 
I1220 08:20:42.745659    3066 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token n9uqvd.ub3ybjnd4zo47yss \
I1220 08:20:42.745728    3066 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:b1099a0a36ba8ecc0c1ce945f60592a322801a8743b416d6f0973f0a83fa88c9 
I1220 08:20:42.746360    3066 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1220 08:20:42.746388    3066 cni.go:84] Creating CNI manager for ""
I1220 08:20:42.746410    3066 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1220 08:20:42.762552    3066 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1220 08:20:42.763849    3066 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1220 08:20:42.772845    3066 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1220 08:20:42.787688    3066 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1220 08:20:42.787733    3066 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1220 08:20:42.787742    3066 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_12_20T08_20_42_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1220 08:20:42.917528    3066 kubeadm.go:1081] duration metric: took 129.828864ms to wait for elevateKubeSystemPrivileges.
I1220 08:20:42.917549    3066 ops.go:34] apiserver oom_adj: -16
I1220 08:20:42.917577    3066 kubeadm.go:406] StartCluster complete in 4m25.82862578s
I1220 08:20:42.917590    3066 settings.go:142] acquiring lock: {Name:mk4da03558d2a078053e5fe0be51ca092144eac3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1220 08:20:42.917696    3066 settings.go:150] Updating kubeconfig:  /home/truong/.kube/config
I1220 08:20:42.918073    3066 lock.go:35] WriteFile acquiring /home/truong/.kube/config: {Name:mk90b85e31434af7fa191c77057d26fc31c30d9d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1220 08:20:42.918247    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1220 08:20:42.918256    3066 global.go:111] Querying for installed drivers using PATH=/home/truong/.minikube/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
I1220 08:20:42.918280    3066 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1220 08:20:42.918302    3066 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1220 08:20:42.918313    3066 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1220 08:20:42.918318    3066 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1220 08:20:42.918325    3066 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1220 08:20:42.918327    3066 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1220 08:20:42.918330    3066 addons.go:240] addon storage-provisioner should already be in state true
I1220 08:20:42.918347    3066 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1220 08:20:42.918360    3066 host.go:66] Checking if "minikube" exists ...
I1220 08:20:42.918553    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:20:42.918666    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:20:42.936746    3066 virtualbox.go:136] virtual box version: 6.1.48r159471
I1220 08:20:42.936767    3066 global.go:122] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.48r159471
}
I1220 08:20:42.936808    3066 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1220 08:20:42.936821    3066 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1220 08:20:42.936863    3066 global.go:122] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1220 08:20:42.952511    3066 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1220 08:20:42.952533    3066 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1220 08:20:42.953772    3066 out.go:177] üîé  Verifying Kubernetes components...
I1220 08:20:42.954863    3066 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1220 08:20:42.975288    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:20:42.975301    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:20:42.976548    3066 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1220 08:20:42.975460    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:20:42.977542    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:20:42.977617    3066 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1220 08:20:42.977645    3066 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1220 08:20:42.977660    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:20:42.978142    3066 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1220 08:20:42.978149    3066 addons.go:240] addon default-storageclass should already be in state true
I1220 08:20:42.978165    3066 host.go:66] Checking if "minikube" exists ...
I1220 08:20:42.978427    3066 main.go:141] libmachine: COMMAND: /usr/bin/VBoxManage showvminfo minikube --machinereadable
I1220 08:20:43.009591    3066 api_server.go:52] waiting for apiserver process to appear ...
I1220 08:20:43.009673    3066 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1220 08:20:43.009746    3066 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.59.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1220 08:20:43.022314    3066 api_server.go:72] duration metric: took 69.757962ms to wait for apiserver process to appear ...
I1220 08:20:43.022326    3066 api_server.go:88] waiting for apiserver healthz status ...
I1220 08:20:43.022338    3066 api_server.go:253] Checking apiserver healthz at https://192.168.59.100:8443/healthz ...
I1220 08:20:43.026455    3066 api_server.go:279] https://192.168.59.100:8443/healthz returned 200:
ok
I1220 08:20:43.027679    3066 api_server.go:141] control plane version: v1.28.3
I1220 08:20:43.027686    3066 api_server.go:131] duration metric: took 5.35675ms to wait for apiserver health ...
I1220 08:20:43.027690    3066 system_pods.go:43] waiting for kube-system pods to appear ...
I1220 08:20:43.030006    3066 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
CfgFile="/home/truong/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/home/truong/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/home/truong/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="226b772b-0ad9-40b1-bd7f-4526aa7107f1"
memory=3900
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2023-12-20T01:15:41.511000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/home/truong/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="738e932f-e311-4438-bc1f-e26302b286f7"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-1-0"="/home/truong/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="5122a3d4-aa8b-4adc-8adf-a703d6981c6c"
"SATA-hot-pluggable"="off"
"SATA-nonrotational"="off"
"SATA-discard"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027EC5B94"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,43605,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027CEE20A"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="pulse"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="hosthome"
SharedFolderPathMachineMapping1="/home"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="/home/truong/.minikube/machines/minikube/minikube/minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1703034960448
GuestAdditionsFacility_VirtualBox System Service=50,1703034960697
GuestAdditionsFacility_Seamless Mode=0,1703034960447
GuestAdditionsFacility_Graphics Mode=0,1703034960447
}
I1220 08:20:43.030022    3066 main.go:141] libmachine: STDERR:
{
}
I1220 08:20:43.030118    3066 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1220 08:20:43.030125    3066 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1220 08:20:43.030135    3066 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:43605 SSHKeyPath:/home/truong/.minikube/machines/minikube/id_rsa Username:docker}
I1220 08:20:43.031633    3066 system_pods.go:59] 4 kube-system pods found
I1220 08:20:43.031641    3066 system_pods.go:61] "etcd-minikube" [3112ccc6-7bce-448c-b8d3-6174bc508a67] Pending
I1220 08:20:43.031643    3066 system_pods.go:61] "kube-apiserver-minikube" [339253ac-18dc-42a0-9141-c35b8f497254] Pending
I1220 08:20:43.031646    3066 system_pods.go:61] "kube-controller-manager-minikube" [70915dc9-f16a-4112-bf21-f8552b6c1ec7] Pending
I1220 08:20:43.031649    3066 system_pods.go:61] "kube-scheduler-minikube" [c79fa31a-3b1e-4004-ae29-82d535edbfd0] Pending
I1220 08:20:43.031652    3066 system_pods.go:74] duration metric: took 3.958636ms to wait for pod list to return data ...
I1220 08:20:43.031657    3066 kubeadm.go:581] duration metric: took 79.108065ms to wait for : map[apiserver:true system_pods:true] ...
I1220 08:20:43.031664    3066 node_conditions.go:102] verifying NodePressure condition ...
I1220 08:20:43.033599    3066 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I1220 08:20:43.033610    3066 node_conditions.go:123] node cpu capacity is 2
I1220 08:20:43.033618    3066 node_conditions.go:105] duration metric: took 1.951928ms to run NodePressure ...
I1220 08:20:43.033637    3066 start.go:228] waiting for startup goroutines ...
I1220 08:20:43.054592    3066 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1220 08:20:43.142126    3066 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1220 08:20:43.758040    3066 start.go:926] {"host.minikube.internal": 192.168.59.1} host record injected into CoreDNS's ConfigMap
I1220 08:20:43.887882    3066 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1220 08:20:43.888987    3066 addons.go:502] enable addons completed in 970.701876ms: enabled=[storage-provisioner default-storageclass]
I1220 08:20:43.889009    3066 start.go:233] waiting for cluster config update ...
I1220 08:20:43.889017    3066 start.go:242] writing updated cluster config ...
I1220 08:20:43.889199    3066 ssh_runner.go:195] Run: rm -f paused
I1220 08:20:43.976617    3066 start.go:600] kubectl: 1.29.0, cluster: 1.28.3 (minor skew: 1)
I1220 08:20:43.977782    3066 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Wed 2023-12-20 01:16:00 UTC, ends at Wed 2023-12-20 03:49:26 UTC. --
Dec 20 03:43:48 minikube dockerd[966]: time="2023-12-20T03:43:48.805535377Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:43:53 minikube dockerd[966]: time="2023-12-20T03:43:53.146348622Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:03 minikube dockerd[966]: time="2023-12-20T03:44:03.250053690Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:13 minikube dockerd[966]: time="2023-12-20T03:44:13.374009944Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:23 minikube dockerd[966]: time="2023-12-20T03:44:23.498511676Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:33 minikube dockerd[966]: time="2023-12-20T03:44:33.620692052Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:43 minikube dockerd[966]: time="2023-12-20T03:44:43.722194468Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.721307780Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.721354028Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.721366119Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.721373127Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.775910350Z" level=info msg="shim disconnected" id=1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d namespace=moby
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.775997806Z" level=warning msg="cleaning up after shim disconnected" id=1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d namespace=moby
Dec 20 03:44:44 minikube dockerd[972]: time="2023-12-20T03:44:44.776006143Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 20 03:44:44 minikube dockerd[966]: time="2023-12-20T03:44:44.776502283Z" level=info msg="ignoring event" container=1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 20 03:44:48 minikube dockerd[966]: time="2023-12-20T03:44:48.852416397Z" level=error msg="Error removing init layer 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:44:48 minikube dockerd[966]: time="2023-12-20T03:44:48.852586401Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:44:53 minikube dockerd[966]: time="2023-12-20T03:44:53.831037211Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:03 minikube dockerd[966]: time="2023-12-20T03:45:03.966485764Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:14 minikube dockerd[966]: time="2023-12-20T03:45:14.089144642Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:24 minikube dockerd[966]: time="2023-12-20T03:45:24.200550262Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:34 minikube dockerd[966]: time="2023-12-20T03:45:34.271553211Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:44 minikube dockerd[966]: time="2023-12-20T03:45:44.407956279Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:45:48 minikube dockerd[966]: time="2023-12-20T03:45:48.911934917Z" level=error msg="Error removing init layer 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:45:48 minikube dockerd[966]: time="2023-12-20T03:45:48.912359348Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:45:54 minikube dockerd[966]: time="2023-12-20T03:45:54.473499105Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:04 minikube dockerd[966]: time="2023-12-20T03:46:04.583269803Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:14 minikube dockerd[966]: time="2023-12-20T03:46:14.731584924Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:24 minikube dockerd[966]: time="2023-12-20T03:46:24.814872727Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:34 minikube dockerd[966]: time="2023-12-20T03:46:34.934986788Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:45 minikube dockerd[966]: time="2023-12-20T03:46:45.034325047Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:46:48 minikube dockerd[966]: time="2023-12-20T03:46:48.935165137Z" level=error msg="Error removing init layer 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:46:48 minikube dockerd[966]: time="2023-12-20T03:46:48.935530454Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:46:55 minikube dockerd[966]: time="2023-12-20T03:46:55.113508882Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:05 minikube dockerd[966]: time="2023-12-20T03:47:05.375283083Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:15 minikube dockerd[966]: time="2023-12-20T03:47:15.624003313Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:25 minikube dockerd[966]: time="2023-12-20T03:47:25.833182786Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.697533993Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.697648771Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.697681087Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.697708170Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.781807420Z" level=info msg="shim disconnected" id=5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2 namespace=moby
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.781880630Z" level=warning msg="cleaning up after shim disconnected" id=5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2 namespace=moby
Dec 20 03:47:32 minikube dockerd[972]: time="2023-12-20T03:47:32.781892300Z" level=info msg="cleaning up dead shim" namespace=moby
Dec 20 03:47:32 minikube dockerd[966]: time="2023-12-20T03:47:32.782152092Z" level=info msg="ignoring event" container=5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 20 03:47:35 minikube dockerd[966]: time="2023-12-20T03:47:35.911082299Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:46 minikube dockerd[966]: time="2023-12-20T03:47:46.006265197Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:47:48 minikube dockerd[966]: time="2023-12-20T03:47:48.954526088Z" level=error msg="Error removing init layer 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:47:48 minikube dockerd[966]: time="2023-12-20T03:47:48.954753324Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:47:56 minikube dockerd[966]: time="2023-12-20T03:47:56.139652674Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:06 minikube dockerd[966]: time="2023-12-20T03:48:06.247072870Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:16 minikube dockerd[966]: time="2023-12-20T03:48:16.327067219Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:26 minikube dockerd[966]: time="2023-12-20T03:48:26.456722274Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:36 minikube dockerd[966]: time="2023-12-20T03:48:36.566081556Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:46 minikube dockerd[966]: time="2023-12-20T03:48:46.703997135Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:48:48 minikube dockerd[966]: time="2023-12-20T03:48:48.972667394Z" level=error msg="Error removing init layer 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:48:48 minikube dockerd[966]: time="2023-12-20T03:48:48.972977705Z" level=error msg="Handler for DELETE /v1.42/containers/0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf returned error: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message"
Dec 20 03:48:56 minikube dockerd[966]: time="2023-12-20T03:48:56.787375230Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:49:06 minikube dockerd[966]: time="2023-12-20T03:49:06.874228384Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"
Dec 20 03:49:16 minikube dockerd[966]: time="2023-12-20T03:49:16.969137235Z" level=error msg="Driver overlay2 couldn't return diff size of container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: stat /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf: no such file or directory"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                             CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
5521e9ec532d3       f564905327305                                                                                                     About a minute ago   Exited              go-base                     6                   63c88ef09bc3e       go-base-deployment-7495f4cc7c-kw955
a47ac7c49411d       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c              2 hours ago          Running             dashboard-metrics-scraper   0                   14d53ccfb0370       dashboard-metrics-scraper-7fd5cb4ddc-hjqxx
2d0283fb1c971       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                    2 hours ago          Running             kubernetes-dashboard        0                   bfa53b905e505       kubernetes-dashboard-8694d4445c-sbqvv
9d52b1bd66f3a       gcr.io/k8s-minikube/kube-registry-proxy@sha256:f107ecd58728a2df5f2bb7e087f65f5363d0019b1e1fd476e4ef16065f44abfb   2 hours ago          Running             registry-proxy              0                   fb7ee3f363597       registry-proxy-v7vlm
a23b3ce7ab7b9       registry@sha256:8a60daaa55ab0df4607c4d8625b96b97b06fd2e6ca8528275472963c4ae8afa0                                  2 hours ago          Running             registry                    0                   7f32e2468bfe3       registry-92667
1f93ecc7247e5       6e38f40d628db                                                                                                     2 hours ago          Running             storage-provisioner         1                   3bb8d0a9d7696       storage-provisioner
bd2660f49440e       6e38f40d628db                                                                                                     2 hours ago          Exited              storage-provisioner         0                   3bb8d0a9d7696       storage-provisioner
5e7c2e006ce77       ead0a4a53df89                                                                                                     2 hours ago          Running             coredns                     0                   93be26d70d2d5       coredns-5dd5756b68-tflnd
5abd59e863d03       bfc896cf80fba                                                                                                     2 hours ago          Running             kube-proxy                  0                   b4e10ac8b0dbb       kube-proxy-4rfvx
fa4022c152017       10baa1ca17068                                                                                                     2 hours ago          Running             kube-controller-manager     0                   8489db202c256       kube-controller-manager-minikube
92632a03b25a2       5374347291230                                                                                                     2 hours ago          Running             kube-apiserver              0                   585cd85affe5c       kube-apiserver-minikube
cae9030bd1b36       73deb9a3f7025                                                                                                     2 hours ago          Running             etcd                        0                   9576a5af4493b       etcd-minikube
d24938f48385d       6d1b4fd1b182d                                                                                                     2 hours ago          Running             kube-scheduler              0                   ec4342c4d4747       kube-scheduler-minikube
0c361525f50f9       6e38f40d628db                                                                                                     19 hours ago         Unknown             storage-provisioner         1                   b76046062a7c2       storage-provisioner

* 
* ==> coredns [5e7c2e006ce7] <==
* [INFO] 10.244.0.17:41394 - 46827 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000191053s
[INFO] 10.244.0.17:48701 - 12333 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000090293s
[INFO] 10.244.0.17:40109 - 13689 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000101375s
[INFO] 10.244.0.17:46752 - 48283 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000048024s
[INFO] 10.244.0.18:42469 - 497 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000158171s
[INFO] 10.244.0.18:57069 - 55411 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.00005465s
[INFO] 10.244.0.18:51451 - 63574 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000508913s
[INFO] 10.244.0.18:35845 - 12510 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.000476168s
[INFO] 10.244.0.18:58007 - 41081 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000351365s
[INFO] 10.244.0.18:45931 - 22860 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000151117s
[INFO] 10.244.0.18:39634 - 7677 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000296303s
[INFO] 10.244.0.18:57346 - 44598 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000046893s
[INFO] 10.244.0.18:47082 - 36930 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000100856s
[INFO] 10.244.0.18:59134 - 46532 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.000036911s
[INFO] 10.244.0.18:55133 - 18942 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000050714s
[INFO] 10.244.0.18:56223 - 22716 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,aa,rd,ra 46 0.000033789s
[INFO] 10.244.0.18:54245 - 6677 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,aa,rd,ra 60 0.000032725s
[INFO] 10.244.0.18:47949 - 48834 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000033949s
[INFO] 10.244.0.18:35102 - 20254 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000023115s
[INFO] 10.244.0.18:36036 - 17263 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000021776s
[INFO] 10.244.0.18:33585 - 8928 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000062877s
[INFO] 10.244.0.18:37103 - 25135 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.00003956s
[INFO] 10.244.0.18:53126 - 4138 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000151007s
[INFO] 10.244.0.18:46222 - 50148 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.000814247s
[INFO] 10.244.0.18:44509 - 444 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000395944s
[INFO] 10.244.0.18:49226 - 26061 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000042901s
[INFO] 10.244.0.18:40848 - 24312 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000032868s
[INFO] 10.244.0.18:54344 - 22014 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000023963s
[INFO] 10.244.0.18:35562 - 27782 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.00009245s
[INFO] 10.244.0.18:52206 - 6395 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.00006523s
[INFO] 10.244.0.18:44434 - 61390 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000064472s
[INFO] 10.244.0.18:55664 - 57984 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.0003187s
[INFO] 10.244.0.18:51575 - 49153 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000365222s
[INFO] 10.244.0.18:57495 - 64532 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000068554s
[INFO] 10.244.0.18:59555 - 46337 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000085797s
[INFO] 10.244.0.18:52340 - 13233 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000062875s
[INFO] 10.244.0.18:58077 - 11334 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.00013583s
[INFO] 10.244.0.18:33040 - 10866 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.000083808s
[INFO] 10.244.0.18:45415 - 34487 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000060042s
[INFO] 10.244.0.18:36372 - 48533 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.000620601s
[INFO] 10.244.0.18:58381 - 37269 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000443958s
[INFO] 10.244.0.18:41335 - 25038 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000081229s
[INFO] 10.244.0.18:46351 - 12502 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000078482s
[INFO] 10.244.0.18:43839 - 19953 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000057118s
[INFO] 10.244.0.18:33415 - 34325 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000126373s
[INFO] 10.244.0.18:52377 - 5390 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.000066024s
[INFO] 10.244.0.18:59339 - 36417 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000065868s
[INFO] 10.244.0.18:52876 - 9996 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.000457863s
[INFO] 10.244.0.18:46751 - 50131 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000321266s
[INFO] 10.244.0.18:34657 - 60320 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000064677s
[INFO] 10.244.0.18:55530 - 5994 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.00004818s
[INFO] 10.244.0.18:47347 - 27492 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000057502s
[INFO] 10.244.0.18:52356 - 52998 "TXT IN cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 83 false 1232" NXDOMAIN qr,aa,rd 165 0.000126415s
[INFO] 10.244.0.18:34606 - 39812 "TXT IN cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 75 false 1232" NXDOMAIN qr,aa,rd 157 0.000076106s
[INFO] 10.244.0.18:54604 - 41418 "TXT IN cluster0.zrvxwix.mongodb.net.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000102272s
[INFO] 10.244.0.18:35185 - 51971 "TXT IN cluster0.zrvxwix.mongodb.net. udp 57 false 1232" NOERROR qr,rd,ra 46 0.000308333s
[INFO] 10.244.0.18:43477 - 51696 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net. udp 71 false 1232" NOERROR qr,rd,ra 60 0.000389622s
[INFO] 10.244.0.18:48886 - 50682 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.default.svc.cluster.local. udp 97 false 1232" NXDOMAIN qr,aa,rd 179 0.000106964s
[INFO] 10.244.0.18:46352 - 33799 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.svc.cluster.local. udp 89 false 1232" NXDOMAIN qr,aa,rd 171 0.000077932s
[INFO] 10.244.0.18:47689 - 23498 "SRV IN _mongodb._tcp.cluster0.zrvxwix.mongodb.net.cluster.local. udp 85 false 1232" NXDOMAIN qr,aa,rd 167 0.000091293s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_12_20T08_20_42_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 20 Dec 2023 01:20:39 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 20 Dec 2023 03:49:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 20 Dec 2023 03:49:18 +0000   Wed, 20 Dec 2023 01:20:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 20 Dec 2023 03:49:18 +0000   Wed, 20 Dec 2023 01:20:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 20 Dec 2023 03:49:18 +0000   Wed, 20 Dec 2023 01:20:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 20 Dec 2023 03:49:18 +0000   Wed, 20 Dec 2023 01:20:46 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.59.100
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3814228Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3814228Ki
  pods:               110
System Info:
  Machine ID:                 7996e76cf2d6418db8392b556a453766
  System UUID:                2b776b22-d90a-b140-bd7f-4526aa7107f1
  Boot ID:                    22e11126-ee61-4934-ae3f-f74fee9509e8
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     go-base-deployment-7495f4cc7c-kw955           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m39s
  kube-system                 coredns-5dd5756b68-tflnd                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     148m
  kube-system                 etcd-minikube                                 100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         148m
  kube-system                 kube-apiserver-minikube                       250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kube-system                 kube-controller-manager-minikube              200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kube-system                 kube-proxy-4rfvx                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kube-system                 kube-scheduler-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kube-system                 registry-92667                                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 registry-proxy-v7vlm                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-hjqxx    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         100m
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-sbqvv         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         100m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000739] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:35] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000511] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:36] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000937] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:37] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000004] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000911] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:38] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000668] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:39] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000934] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000024] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:40] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000512] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:41] kauditd_printk_skb: 5 callbacks suppressed
[ +21.720644] kauditd_printk_skb: 4 callbacks suppressed
[  +0.618778] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000662] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:42] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000004] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.001205] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:43] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000522] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:44] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000866] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:45] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.002274] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:46] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000655] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:47] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000467] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum
[Dec20 03:48] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055352: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055352: comm dockerd: Directory block failed checksum
[  +0.000551] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #1055347: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #1055347: comm dockerd: Directory block failed checksum

* 
* ==> etcd [cae9030bd1b3] <==
* {"level":"info","ts":"2023-12-20T02:10:38.699159Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2646}
{"level":"info","ts":"2023-12-20T02:10:38.699851Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2646,"took":"381.03¬µs","hash":2904211771}
{"level":"info","ts":"2023-12-20T02:10:38.700031Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2904211771,"revision":2646,"compact-revision":2405}
{"level":"info","ts":"2023-12-20T02:15:38.705533Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2966}
{"level":"info","ts":"2023-12-20T02:15:38.706838Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2966,"took":"976.234¬µs","hash":2885314311}
{"level":"info","ts":"2023-12-20T02:15:38.70687Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2885314311,"revision":2966,"compact-revision":2646}
{"level":"info","ts":"2023-12-20T02:20:38.720396Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3209}
{"level":"info","ts":"2023-12-20T02:20:38.727035Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3209,"took":"4.126091ms","hash":2170377376}
{"level":"info","ts":"2023-12-20T02:20:38.727203Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2170377376,"revision":3209,"compact-revision":2966}
{"level":"info","ts":"2023-12-20T02:25:38.728463Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3451}
{"level":"info","ts":"2023-12-20T02:25:38.731593Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3451,"took":"2.733017ms","hash":958464448}
{"level":"info","ts":"2023-12-20T02:25:38.731661Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":958464448,"revision":3451,"compact-revision":3209}
{"level":"info","ts":"2023-12-20T02:30:38.739566Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3697}
{"level":"info","ts":"2023-12-20T02:30:38.742848Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3697,"took":"2.618852ms","hash":3972111442}
{"level":"info","ts":"2023-12-20T02:30:38.742961Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3972111442,"revision":3697,"compact-revision":3451}
{"level":"info","ts":"2023-12-20T02:35:38.74321Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3939}
{"level":"info","ts":"2023-12-20T02:35:38.744097Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3939,"took":"468.116¬µs","hash":3583272130}
{"level":"info","ts":"2023-12-20T02:35:38.744171Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3583272130,"revision":3939,"compact-revision":3697}
{"level":"info","ts":"2023-12-20T02:40:38.746447Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4182}
{"level":"info","ts":"2023-12-20T02:40:38.747086Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4182,"took":"338.668¬µs","hash":1601742664}
{"level":"info","ts":"2023-12-20T02:40:38.747131Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1601742664,"revision":4182,"compact-revision":3939}
{"level":"info","ts":"2023-12-20T02:45:38.751984Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4426}
{"level":"info","ts":"2023-12-20T02:45:38.753474Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4426,"took":"1.03276ms","hash":3734107388}
{"level":"info","ts":"2023-12-20T02:45:38.753525Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3734107388,"revision":4426,"compact-revision":4182}
{"level":"info","ts":"2023-12-20T02:50:38.762865Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4668}
{"level":"info","ts":"2023-12-20T02:50:38.766316Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4668,"took":"2.673497ms","hash":2482948227}
{"level":"info","ts":"2023-12-20T02:50:38.766502Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2482948227,"revision":4668,"compact-revision":4426}
{"level":"info","ts":"2023-12-20T02:55:38.775097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4913}
{"level":"info","ts":"2023-12-20T02:55:38.779869Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4913,"took":"2.79426ms","hash":4091522795}
{"level":"info","ts":"2023-12-20T02:55:38.780091Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4091522795,"revision":4913,"compact-revision":4668}
{"level":"info","ts":"2023-12-20T03:00:38.784845Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5155}
{"level":"info","ts":"2023-12-20T03:00:38.787887Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5155,"took":"2.074416ms","hash":3166058344}
{"level":"info","ts":"2023-12-20T03:00:38.788602Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3166058344,"revision":5155,"compact-revision":4913}
{"level":"info","ts":"2023-12-20T03:05:38.794968Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5398}
{"level":"info","ts":"2023-12-20T03:05:38.806159Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5398,"took":"2.006051ms","hash":2124242126}
{"level":"info","ts":"2023-12-20T03:05:38.806436Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2124242126,"revision":5398,"compact-revision":5155}
{"level":"info","ts":"2023-12-20T03:10:38.804377Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5650}
{"level":"info","ts":"2023-12-20T03:10:38.807609Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5650,"took":"1.906371ms","hash":2982240107}
{"level":"info","ts":"2023-12-20T03:10:38.808289Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2982240107,"revision":5650,"compact-revision":5398}
{"level":"info","ts":"2023-12-20T03:15:38.81582Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5941}
{"level":"info","ts":"2023-12-20T03:15:38.823934Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5941,"took":"6.844019ms","hash":4122626632}
{"level":"info","ts":"2023-12-20T03:15:38.824603Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4122626632,"revision":5941,"compact-revision":5650}
{"level":"info","ts":"2023-12-20T03:20:38.830354Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6191}
{"level":"info","ts":"2023-12-20T03:20:38.834137Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6191,"took":"3.054672ms","hash":4187640696}
{"level":"info","ts":"2023-12-20T03:20:38.834928Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4187640696,"revision":6191,"compact-revision":5941}
{"level":"info","ts":"2023-12-20T03:25:38.838194Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6492}
{"level":"info","ts":"2023-12-20T03:25:38.83958Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6492,"took":"1.019014ms","hash":2259198795}
{"level":"info","ts":"2023-12-20T03:25:38.839616Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2259198795,"revision":6492,"compact-revision":6191}
{"level":"info","ts":"2023-12-20T03:30:38.846415Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6802}
{"level":"info","ts":"2023-12-20T03:30:38.847075Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6802,"took":"452.389¬µs","hash":1104476659}
{"level":"info","ts":"2023-12-20T03:30:38.847107Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1104476659,"revision":6802,"compact-revision":6492}
{"level":"info","ts":"2023-12-20T03:35:38.856655Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7113}
{"level":"info","ts":"2023-12-20T03:35:38.861363Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7113,"took":"3.048882ms","hash":1792777050}
{"level":"info","ts":"2023-12-20T03:35:38.861646Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1792777050,"revision":7113,"compact-revision":6802}
{"level":"info","ts":"2023-12-20T03:40:38.868754Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7358}
{"level":"info","ts":"2023-12-20T03:40:38.874145Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7358,"took":"4.399542ms","hash":3273596016}
{"level":"info","ts":"2023-12-20T03:40:38.874363Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3273596016,"revision":7358,"compact-revision":7113}
{"level":"info","ts":"2023-12-20T03:45:38.873691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7604}
{"level":"info","ts":"2023-12-20T03:45:38.874275Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7604,"took":"361.207¬µs","hash":3797513550}
{"level":"info","ts":"2023-12-20T03:45:38.874301Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3797513550,"revision":7604,"compact-revision":7358}

* 
* ==> kernel <==
*  03:49:26 up  2:33,  0 users,  load average: 1.15, 0.69, 0.52
Linux minikube 5.10.57 #1 SMP Tue Nov 7 06:51:54 UTC 2023 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [92632a03b25a] <==
* I1220 01:20:39.544868       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1220 01:20:39.544962       1 system_namespaces_controller.go:67] Starting system namespaces controller
I1220 01:20:39.553039       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1220 01:20:39.553082       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I1220 01:20:39.553170       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I1220 01:20:39.553352       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1220 01:20:39.553417       1 controller.go:78] Starting OpenAPI AggregationController
I1220 01:20:39.553456       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1220 01:20:39.553559       1 controller.go:116] Starting legacy_token_tracking_controller
I1220 01:20:39.553607       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1220 01:20:39.553949       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1220 01:20:39.554042       1 available_controller.go:423] Starting AvailableConditionController
I1220 01:20:39.554097       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1220 01:20:39.554119       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I1220 01:20:39.554262       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1220 01:20:39.554314       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1220 01:20:39.577690       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1220 01:20:39.577832       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1220 01:20:39.578048       1 controller.go:134] Starting OpenAPI controller
I1220 01:20:39.578096       1 controller.go:85] Starting OpenAPI V3 controller
I1220 01:20:39.578151       1 naming_controller.go:291] Starting NamingConditionController
I1220 01:20:39.578193       1 establishing_controller.go:76] Starting EstablishingController
I1220 01:20:39.578229       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1220 01:20:39.578302       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1220 01:20:39.578337       1 crd_finalizer.go:266] Starting CRDFinalizer
I1220 01:20:39.553183       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1220 01:20:39.578415       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1220 01:20:39.644786       1 apf_controller.go:377] Running API Priority and Fairness config worker
I1220 01:20:39.644804       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I1220 01:20:39.646358       1 controller.go:624] quota admission added evaluator for: namespaces
I1220 01:20:39.652882       1 shared_informer.go:318] Caches are synced for node_authorizer
I1220 01:20:39.654070       1 shared_informer.go:318] Caches are synced for configmaps
I1220 01:20:39.654432       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1220 01:20:39.654758       1 aggregator.go:166] initial CRD sync complete...
I1220 01:20:39.654911       1 autoregister_controller.go:141] Starting autoregister controller
I1220 01:20:39.654941       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1220 01:20:39.654953       1 cache.go:39] Caches are synced for autoregister controller
I1220 01:20:39.654506       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1220 01:20:39.678124       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1220 01:20:39.678528       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1220 01:20:39.693325       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1220 01:20:40.551448       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1220 01:20:40.555475       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1220 01:20:40.555503       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1220 01:20:40.823906       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1220 01:20:40.843465       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1220 01:20:40.956184       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1220 01:20:40.967543       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.59.100]
I1220 01:20:40.971053       1 controller.go:624] quota admission added evaluator for: endpoints
I1220 01:20:40.982768       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1220 01:20:41.591519       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1220 01:20:42.520128       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1220 01:20:42.535506       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1220 01:20:42.543904       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1220 01:20:54.793401       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1220 01:20:55.255352       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I1220 01:39:14.677693       1 alloc.go:330] "allocated clusterIPs" service="default/go-base" clusterIPs={"IPv4":"10.96.174.233"}
I1220 01:48:15.890374       1 alloc.go:330] "allocated clusterIPs" service="kube-system/registry" clusterIPs={"IPv4":"10.101.47.96"}
I1220 02:08:51.723541       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.96.218.65"}
I1220 02:08:51.754683       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.103.5.16"}

* 
* ==> kube-controller-manager [fa4022c15201] <==
* I1220 03:28:38.954999       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="10.734134ms"
I1220 03:28:38.955053       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="23.507¬µs"
I1220 03:28:38.955128       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="50.235¬µs"
I1220 03:28:39.014877       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-57c7559699" duration="26.124¬µs"
I1220 03:28:39.994763       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-57c7559699" duration="32.217¬µs"
I1220 03:28:40.007205       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-57c7559699" duration="23.47¬µs"
I1220 03:28:40.009321       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-57c7559699" duration="18.55¬µs"
I1220 03:28:40.019452       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="32.338¬µs"
I1220 03:28:41.074075       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="34.128¬µs"
I1220 03:28:52.685894       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="90¬µs"
I1220 03:28:53.368801       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="28.488¬µs"
I1220 03:29:03.657526       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="57.752¬µs"
I1220 03:29:19.074384       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="55.047¬µs"
I1220 03:29:31.677201       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="176.279¬µs"
I1220 03:30:10.365476       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="69.541¬µs"
I1220 03:30:21.673942       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="74.78¬µs"
I1220 03:31:39.840586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="61.823¬µs"
I1220 03:31:53.677797       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="107.295¬µs"
I1220 03:34:30.587733       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="94.721¬µs"
I1220 03:34:45.656428       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="71.572¬µs"
I1220 03:39:37.836834       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="43.302¬µs"
I1220 03:39:49.679873       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="107.729¬µs"
I1220 03:41:26.371495       1 event.go:307] "Event occurred" object="default/go-base-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set go-base-deployment-767c6fd9b6 to 0 from 1"
I1220 03:41:26.375445       1 event.go:307] "Event occurred" object="default/go-base-deployment-767c6fd9b6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: go-base-deployment-767c6fd9b6-k79wv"
I1220 03:41:26.381367       1 event.go:307] "Event occurred" object="default/go-base-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set go-base-deployment-7495f4cc7c to 0 from 1"
I1220 03:41:26.392712       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="21.659358ms"
I1220 03:41:26.403988       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="11.247771ms"
I1220 03:41:26.404128       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="21.628¬µs"
I1220 03:41:26.408682       1 event.go:307] "Event occurred" object="default/go-base-deployment-7495f4cc7c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: go-base-deployment-7495f4cc7c-fwhdn"
I1220 03:41:26.408816       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="38.476¬µs"
I1220 03:41:26.424473       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="43.405125ms"
I1220 03:41:26.442887       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="18.382567ms"
I1220 03:41:26.444528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="28.8¬µs"
I1220 03:41:26.530322       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="35.79¬µs"
I1220 03:41:26.546141       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="28.906¬µs"
I1220 03:41:27.480149       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="22.044¬µs"
I1220 03:41:27.485493       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="49.565¬µs"
I1220 03:41:27.486057       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-767c6fd9b6" duration="25.293¬µs"
I1220 03:41:27.496271       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="31.868¬µs"
I1220 03:41:27.499802       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="19.603¬µs"
I1220 03:41:27.502011       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="14.995¬µs"
I1220 03:41:47.444622       1 event.go:307] "Event occurred" object="default/go-base-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set go-base-deployment-7495f4cc7c to 1 from 0"
I1220 03:41:47.453819       1 event.go:307] "Event occurred" object="default/go-base-deployment-7495f4cc7c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: go-base-deployment-7495f4cc7c-kw955"
I1220 03:41:47.460462       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="16.278733ms"
I1220 03:41:47.474947       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="14.336128ms"
I1220 03:41:47.475188       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="21.628¬µs"
I1220 03:41:47.475296       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="12.953¬µs"
I1220 03:41:49.165625       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="29.834¬µs"
I1220 03:41:50.210297       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="46.644¬µs"
I1220 03:41:51.259148       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="34.521¬µs"
I1220 03:42:03.570035       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="29.514¬µs"
I1220 03:42:14.679560       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="52.181¬µs"
I1220 03:42:28.148725       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="58.5¬µs"
I1220 03:42:41.683746       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="50.076¬µs"
I1220 03:43:20.404961       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="51.111¬µs"
I1220 03:43:30.677143       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="72.892¬µs"
I1220 03:44:45.094138       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="51.151¬µs"
I1220 03:44:56.658125       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="89.757¬µs"
I1220 03:47:32.999106       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="51.654¬µs"
I1220 03:47:47.655488       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/go-base-deployment-7495f4cc7c" duration="49.586¬µs"

* 
* ==> kube-proxy [5abd59e863d0] <==
* I1220 01:20:56.171354       1 server_others.go:69] "Using iptables proxy"
I1220 01:20:56.185672       1 node.go:141] Successfully retrieved node IP: 192.168.59.100
I1220 01:20:56.247996       1 server_others.go:121] "No iptables support for family" ipFamily="IPv6"
I1220 01:20:56.248020       1 server.go:634] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1220 01:20:56.249502       1 server_others.go:152] "Using iptables Proxier"
I1220 01:20:56.249691       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1220 01:20:56.250302       1 server.go:846] "Version info" version="v1.28.3"
I1220 01:20:56.250319       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1220 01:20:56.251879       1 config.go:188] "Starting service config controller"
I1220 01:20:56.252191       1 shared_informer.go:311] Waiting for caches to sync for service config
I1220 01:20:56.252214       1 config.go:315] "Starting node config controller"
I1220 01:20:56.252218       1 shared_informer.go:311] Waiting for caches to sync for node config
I1220 01:20:56.252960       1 config.go:97] "Starting endpoint slice config controller"
I1220 01:20:56.252977       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1220 01:20:56.353112       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1220 01:20:56.353244       1 shared_informer.go:318] Caches are synced for node config
I1220 01:20:56.353261       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [d24938f48385] <==
* I1220 01:20:38.053811       1 serving.go:348] Generated self-signed cert in-memory
W1220 01:20:39.585244       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1220 01:20:39.585372       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1220 01:20:39.585423       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1220 01:20:39.585457       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1220 01:20:39.600962       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1220 01:20:39.601187       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1220 01:20:39.604636       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1220 01:20:39.604841       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1220 01:20:39.604922       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1220 01:20:39.604966       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1220 01:20:39.612580       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1220 01:20:39.612611       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1220 01:20:39.612668       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1220 01:20:39.612716       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1220 01:20:39.612769       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1220 01:20:39.612787       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1220 01:20:39.612820       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:39.612836       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:39.612869       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:39.612897       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:39.612930       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1220 01:20:39.612946       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1220 01:20:39.612975       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1220 01:20:39.612989       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1220 01:20:39.613019       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:39.613035       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:39.613069       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1220 01:20:39.613085       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1220 01:20:39.613111       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1220 01:20:39.613124       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1220 01:20:39.613179       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1220 01:20:39.613196       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1220 01:20:39.613231       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1220 01:20:39.613450       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1220 01:20:39.613532       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:39.613549       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:39.613998       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1220 01:20:39.614099       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1220 01:20:39.614132       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1220 01:20:39.614279       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1220 01:20:40.450931       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:40.450973       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:40.508049       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1220 01:20:40.508184       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1220 01:20:40.510644       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1220 01:20:40.510756       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1220 01:20:40.522625       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1220 01:20:40.522770       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1220 01:20:40.604629       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1220 01:20:40.604666       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1220 01:20:40.674000       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1220 01:20:40.674130       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I1220 01:20:42.306199       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Wed 2023-12-20 01:16:00 UTC, ends at Wed 2023-12-20 03:49:26 UTC. --
Dec 20 03:45:46 minikube kubelet[12249]: I1220 03:45:46.649700   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:45:46 minikube kubelet[12249]: E1220 03:45:46.649884   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:45:48 minikube kubelet[12249]: I1220 03:45:48.885755   12249 scope.go:117] "RemoveContainer" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:45:48 minikube kubelet[12249]: E1220 03:45:48.914262   12249 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:45:48 minikube kubelet[12249]: E1220 03:45:48.914315   12249 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:45:57 minikube kubelet[12249]: I1220 03:45:57.649965   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:45:57 minikube kubelet[12249]: E1220 03:45:57.651967   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:46:12 minikube kubelet[12249]: I1220 03:46:12.649033   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:46:12 minikube kubelet[12249]: E1220 03:46:12.649239   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:46:25 minikube kubelet[12249]: I1220 03:46:25.648510   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:46:25 minikube kubelet[12249]: E1220 03:46:25.649227   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:46:40 minikube kubelet[12249]: I1220 03:46:40.649312   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:46:40 minikube kubelet[12249]: E1220 03:46:40.649880   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:46:42 minikube kubelet[12249]: E1220 03:46:42.670959   12249 iptables.go:575] "Could not set up iptables canary" err=<
Dec 20 03:46:42 minikube kubelet[12249]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 20 03:46:42 minikube kubelet[12249]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 20 03:46:42 minikube kubelet[12249]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 20 03:46:48 minikube kubelet[12249]: I1220 03:46:48.926699   12249 scope.go:117] "RemoveContainer" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:46:48 minikube kubelet[12249]: E1220 03:46:48.935900   12249 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:46:48 minikube kubelet[12249]: E1220 03:46:48.935954   12249 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:46:52 minikube kubelet[12249]: I1220 03:46:52.648866   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:46:52 minikube kubelet[12249]: E1220 03:46:52.649008   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:47:06 minikube kubelet[12249]: I1220 03:47:06.650881   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:47:06 minikube kubelet[12249]: E1220 03:47:06.651988   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:47:18 minikube kubelet[12249]: I1220 03:47:18.649577   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:47:18 minikube kubelet[12249]: E1220 03:47:18.651557   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:47:32 minikube kubelet[12249]: I1220 03:47:32.649965   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:47:32 minikube kubelet[12249]: I1220 03:47:32.991276   12249 scope.go:117] "RemoveContainer" containerID="1eeab8213064e043814f5b049b4ee057a73429a904ab15c31c7244c1efeaa82d"
Dec 20 03:47:32 minikube kubelet[12249]: I1220 03:47:32.991595   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:47:32 minikube kubelet[12249]: E1220 03:47:32.992015   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:47:42 minikube kubelet[12249]: E1220 03:47:42.668349   12249 iptables.go:575] "Could not set up iptables canary" err=<
Dec 20 03:47:42 minikube kubelet[12249]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 20 03:47:42 minikube kubelet[12249]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 20 03:47:42 minikube kubelet[12249]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 20 03:47:47 minikube kubelet[12249]: I1220 03:47:47.648522   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:47:47 minikube kubelet[12249]: E1220 03:47:47.650334   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:47:48 minikube kubelet[12249]: I1220 03:47:48.945804   12249 scope.go:117] "RemoveContainer" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:47:48 minikube kubelet[12249]: E1220 03:47:48.955300   12249 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:47:48 minikube kubelet[12249]: E1220 03:47:48.955399   12249 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:48:01 minikube kubelet[12249]: I1220 03:48:01.649580   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:48:01 minikube kubelet[12249]: E1220 03:48:01.652270   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:48:12 minikube kubelet[12249]: I1220 03:48:12.657951   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:48:12 minikube kubelet[12249]: E1220 03:48:12.659988   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:48:25 minikube kubelet[12249]: I1220 03:48:25.649527   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:48:25 minikube kubelet[12249]: E1220 03:48:25.651059   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:48:40 minikube kubelet[12249]: I1220 03:48:40.648739   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:48:40 minikube kubelet[12249]: E1220 03:48:40.649278   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:48:42 minikube kubelet[12249]: E1220 03:48:42.679286   12249 iptables.go:575] "Could not set up iptables canary" err=<
Dec 20 03:48:42 minikube kubelet[12249]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Dec 20 03:48:42 minikube kubelet[12249]:         Perhaps ip6tables or your kernel needs to be upgraded.
Dec 20 03:48:42 minikube kubelet[12249]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Dec 20 03:48:48 minikube kubelet[12249]: I1220 03:48:48.965575   12249 scope.go:117] "RemoveContainer" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:48:48 minikube kubelet[12249]: E1220 03:48:48.973312   12249 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:48:48 minikube kubelet[12249]: E1220 03:48:48.973350   12249 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf\": Error response from daemon: container 0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/f1efd9dcead66f41699624d6c00119599f64c55f8401ca87ffc71d3332a7fccf-init/diff/proc: readdirent proc: bad message" containerID="0c361525f50f9cce2b63a4d0b668cabdfdcce880b0e1f338fd6f4aac0f9dcbdf"
Dec 20 03:48:52 minikube kubelet[12249]: I1220 03:48:52.649109   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:48:52 minikube kubelet[12249]: E1220 03:48:52.649283   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:49:03 minikube kubelet[12249]: I1220 03:49:03.649088   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:49:03 minikube kubelet[12249]: E1220 03:49:03.649400   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"
Dec 20 03:49:14 minikube kubelet[12249]: I1220 03:49:14.649451   12249 scope.go:117] "RemoveContainer" containerID="5521e9ec532d37e2e43d08ba7b933f13738c2b831f681acd3810129f7a4948f2"
Dec 20 03:49:14 minikube kubelet[12249]: E1220 03:49:14.651494   12249 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-base\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=go-base pod=go-base-deployment-7495f4cc7c-kw955_default(f908c69b-4a80-4047-bc96-3f0f83cfc235)\"" pod="default/go-base-deployment-7495f4cc7c-kw955" podUID="f908c69b-4a80-4047-bc96-3f0f83cfc235"

* 
* ==> kubernetes-dashboard [2d0283fb1c97] <==
* 2023/12/20 02:16:04 [2023-12-20T02:16:04Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:04 received 0 resources from sidecar instead of 1
2023/12/20 02:16:04 received 0 resources from sidecar instead of 1
2023/12/20 02:16:04 [2023-12-20T02:16:04Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:09 [2023-12-20T02:16:09Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:16:09 Getting list of all deployments in the cluster
2023/12/20 02:16:09 [2023-12-20T02:16:09Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:16:09 Getting list of namespaces
2023/12/20 02:16:09 [2023-12-20T02:16:09Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:09 received 0 resources from sidecar instead of 1
2023/12/20 02:16:09 received 0 resources from sidecar instead of 1
2023/12/20 02:16:09 [2023-12-20T02:16:09Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:14 [2023-12-20T02:16:14Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:16:14 Getting list of namespaces
2023/12/20 02:16:14 [2023-12-20T02:16:14Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:16:14 Getting list of all deployments in the cluster
2023/12/20 02:16:14 [2023-12-20T02:16:14Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:14 received 0 resources from sidecar instead of 1
2023/12/20 02:16:14 received 0 resources from sidecar instead of 1
2023/12/20 02:16:14 [2023-12-20T02:16:14Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:19 [2023-12-20T02:16:19Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:16:19 Getting list of namespaces
2023/12/20 02:16:19 [2023-12-20T02:16:19Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:16:19 Getting list of all deployments in the cluster
2023/12/20 02:16:19 [2023-12-20T02:16:19Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:19 received 0 resources from sidecar instead of 1
2023/12/20 02:16:19 received 0 resources from sidecar instead of 1
2023/12/20 02:16:19 [2023-12-20T02:16:19Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:30 [2023-12-20T02:16:30Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:16:30 Getting list of namespaces
2023/12/20 02:16:30 [2023-12-20T02:16:30Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:16:30 Getting list of all deployments in the cluster
2023/12/20 02:16:30 [2023-12-20T02:16:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:30 received 0 resources from sidecar instead of 1
2023/12/20 02:16:30 received 0 resources from sidecar instead of 1
2023/12/20 02:16:30 [2023-12-20T02:16:30Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:35 [2023-12-20T02:16:35Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:16:35 Getting list of namespaces
2023/12/20 02:16:35 [2023-12-20T02:16:35Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:16:35 Getting list of all deployments in the cluster
2023/12/20 02:16:35 [2023-12-20T02:16:35Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:16:35 received 0 resources from sidecar instead of 1
2023/12/20 02:16:35 received 0 resources from sidecar instead of 1
2023/12/20 02:16:35 [2023-12-20T02:16:35Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:24:08 [2023-12-20T02:24:08Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:24:08 Getting list of namespaces
2023/12/20 02:24:08 [2023-12-20T02:24:08Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:24:08 Getting list of all deployments in the cluster
2023/12/20 02:24:08 [2023-12-20T02:24:08Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:24:08 received 0 resources from sidecar instead of 1
2023/12/20 02:24:08 received 0 resources from sidecar instead of 1
2023/12/20 02:24:08 [2023-12-20T02:24:08Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:24:11 [2023-12-20T02:24:11Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/20 02:24:11 Getting list of namespaces
2023/12/20 02:24:11 [2023-12-20T02:24:11Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/20 02:24:12 [2023-12-20T02:24:12Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/20 02:24:12 Getting list of all deployments in the cluster
2023/12/20 02:24:12 received 0 resources from sidecar instead of 1
2023/12/20 02:24:12 received 0 resources from sidecar instead of 1
2023/12/20 02:24:12 [2023-12-20T02:24:12Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [0c361525f50f] <==
* 
* ==> storage-provisioner [1f93ecc7247e] <==
* I1220 01:21:26.936741       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1220 01:21:26.949525       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1220 01:21:26.949741       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1220 01:21:26.955159       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1220 01:21:26.955717       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_796949aa-1e0c-4207-8abf-8b533cf904fa!
I1220 01:21:26.957059       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8b742faf-1b50-4e42-9130-2fd78dedeb55", APIVersion:"v1", ResourceVersion:"422", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_796949aa-1e0c-4207-8abf-8b533cf904fa became leader
I1220 01:21:27.056382       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_796949aa-1e0c-4207-8abf-8b533cf904fa!

* 
* ==> storage-provisioner [bd2660f49440] <==
* I1220 01:20:56.421121       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1220 01:21:26.426422       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

